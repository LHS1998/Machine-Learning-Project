**第三次实验报告**

**李瀚笙 1338875**      

[TOC]

# 一、实验目标

​		本实验以 [Trec06](https://plg.uwaterloo.ca/cgi-bin/cgiwrap/gvcormac/foo06) 的中文垃圾邮件数据集为基础，用贝叶斯学习的方法建立一个垃圾邮件分类器。

​		本实验报告中所有代码均为示例代码，为了叙述清晰，部分是从函数、类中粘贴出来，不保证单独可以正确执行。作者所使用的环境是 MacOS 12.2.1，Python 3.9，PyCharm 2021.3.1，能正确运行各脚本。



# 二、数据描述与处理

### 2.1 邮件信息读入

​		注意到标签文件中含有各邮件的存储位置，因此以它为起点分别读取。

```python
with open("trec06c-utf8/label/index", mode='rt') as f:
    input_index = f.read()

indexes = input_index.split('\n')
for index in indexes:
    label, path = index.split(" ")
    address = path[7:]  # 去掉 ../data
    with open("trec06c-utf8/data" + address, mode='rt') as f:
        uncut = f.read()
```

这样就可以依次把每封邮件的标签和内容都读入内存了。

### 2.2 邮件头处理

​		通过观察，可以用换行对邮件进行划分，头和主体之间是 \n\n (即 b'\x0a\x0a')，而某些邮件头有多行，次行起以 \n\t (即 b'\x0a\x09') 开头。处理代码如下：

```python
for index in indexes:
    label, path = index.split(" ")
    address = path[7:]  # 去掉 ../data
    with open("trec06c-utf8/data" + address, mode='rt') as f:
        uncut = f.read()
    parts = uncut.split('\n\n')
    head = parts[0]
    trunk = '\n\n'.join(parts[1:])
    lines = head.split('\n')
    entry = ""
    header = {}
    for line in lines:  # 拆文件头
        if line.startswith("\t") or line.startswith('    '):  # 继续上一条 entry
            header[entry] += "\n" + line.lstrip()
        else:  # 新 entry
            pars = line.split(": ")  # 防止时间遭拆分
            entry = pars[0]
            header[entry] = ': '.join(pars[1:])
    print(header)
    break
```

打印出首封邮件头如下：

```python
{
  	'Received': 'from hp-5e1fe6310264 ([218.79.188.136])\nby spam-gw.ccert.edu.cn (MIMEDefang) with ESMTP id j7CAoGvt023247\nfor <lu@ccert.edu.cn>; Sun, 14 Aug 2005 09:59:04 +0800 (CST)', 
  	'Message-ID': '<200508121850.j7CAoGvt023247@spam-gw.ccert.edu.cn>', 
  	'From': '"yan"<(8月27-28,上海)培训课程>', 
  	'Reply-To': 'yan@vip.163.com"<b4a7r0h0@vip.163.com>', 
  	'To': 'lu@ccert.edu.cn', 
  	'Subject': '=?gb2312?B?t8eyxs7xvq3A7bXEssbO8bncwO0to6jJs8XMxKPE4qOp?=', 
  	'Date': 'Tue, 30 Aug 2005 10:08:15 +0800', 
  	'MIME-Version': '1.0', 
  	'Content-type': 'multipart/related;\ntype="multipart/alternative";\nboundary="----=_NextPart_000_004A_2531AAAC.6F950005"', 
  	'X-Priority': '3', 
  	'X-MSMail-Priority': 'Normal', 
  	'X-Mailer': 'Microsoft Outlook Express 6.00.2800.1158', 
  	'X-MimeOLE': 'Produced By Microsoft MimeOLE V6.00.2800.1441'
}
```

由此可以提取一些重要信息，比如发件方 ip 地址保存在 Received 条目中，发件方邮箱保存在 Reply-To 或 From 中，邮件主题是 MIME 编码后保存在 Subject 条目中，日期保存在 Date 和 Received 条目中，这三条是最重要的信息。根据第一篇邮件，设计以下类用于提取信息：

```python
class Mail:
    def __init__(self, label, address, header, trunk):
        self.label = label
        self.address = address
        self.from_name = header["From"].split('<')[1].split('>')[0]
        self.from_add = header["Reply-To"].split('<')[1].split('>')[0]
        self.to_ = header["To"]
        self.subject_ = header["Subject"]
        self.date_ = header["Date"]
        self.type_ = header["Content-type"].split(";")[0]
        self.mailer_ = header["X-Mailer"]
        self.ip_ = header["Received"].split("[")[1].split("]")[0]
        self.trunk = trunk
```

但是不同的发件方、收件方形成的邮件头格式均不相同，导致上述代码在遇到特定邮件时会出现字典查找的 KeyError 和划分字节后的列表的 IndexError。因为只涉及这一特定任务的处理，不妨边调试边修改。在以下循环中调试：

```python
for index in indexes[len(mail_data):]:
    label, path = index.split(" ")
    address = path[7:]  # 去掉 ../data
    with open("trec06c-utf8/data" + address, mode='rt') as f:
        uncut = f.read()
    parts = uncut.split('\n\n')
    head = parts[0]
    trunk = '\n\n'.join(parts[1:])
    lines = head.split('\n')
    entry = ""
    header = {}
    for line in lines:  # 拆文件头
        if line.startswith("\t") or line.startswith('    '):  # 继续上一条 entry
            header[entry] += "\n" + line.lstrip()
        else:  # 新 entry
            pars = line.split(": ")  # 防止时间遭拆分
            entry = pars[0]
            header[entry] = ': '.join(pars[1:])
    this_mail = Mail(label, address, header, trunk, lac)
    mail_data.append(this_mail)
```

此时停止后打印 header 即可打印出错位置的头部信息以供调试，下次运行时则恰好从中断的条目处继续运行，可以节省重新运行的时间。处理 bug 的方法是遇到 KeyError 就把取值改成 setdefault，遇到IndexError 就加一个 try，然后根据出错位置的头文件判断所需信息具体在何处，没有相关信息则设为 default。循环处理完后，上述类变成了以下形式：

```python
from email.header import decode_header


class Mail:
    def __init__(self, label, address, header, trunk):
        # 输入信息
        self.label = label
        self.address = address

        # 邮件头处理
        if header.setdefault("From", '"default').startswith("=?"):
            s, codec = decode_header(header["From"])[0]
            try:
                header["From"] = s.decode(codec)
            except UnicodeDecodeError:
                header["From"] = s.decode('gbk', errors='replace')
        try:
            self.from_name = header["From"].split('"')[1]  # 中文的昵称编码储存，没有引号
        except IndexError:
            self.from_name = header["From"].split('<')[0]
        try:
            from_add = header["From"].split('<')[1].split('>')[0]
        except IndexError:
            from_add = header["From"]
        if from_add.find("@") == -1:
            try:
                from_add = header.setdefault("Reply-To", "<default>").split('<')[1].split('>')[0]
            except IndexError:
                from_add = header["Reply-To"]
        self.from_add = from_add
        try:
            self.to_ = header["To"]
        except KeyError:
            self.to_ = header["TO"]
        if header["Subject"].startswith("=?"):
            s, codec = decode_header(header["Subject"])[0]
            try:
                self.subject_ = s.decode(codec)
            except UnicodeDecodeError:
                self.subject_ = s.decode('gbk', errors='replace')
        else:
            self.subject_ = header["Subject"]
        try:
            self.date_ = header["Date"]
        except KeyError:
            self.date_ = header["Received"].split("; ")[-1]
        self.type_ = header.setdefault("Content-type", "default;").split(";")[0]
        self.mailer_ = header.setdefault("X-Mailer", "default")
        try:
            self.ip_ = header["Received"].split("[")[1].split("]")[0]  # ip 不都是以 [] 包围的
        except IndexError:
            ips = header["Received"].split(".")  # 这个方法仍然存在误读
            while True:
                offset = 0
                try:
                    pp = (ips[offset][-3:], ips[offset+1], ips[offset+2], ips[offset+3][:3])
                except IndexError:
                    self.ip_ = 'not_found'
                    break
                violate = False
                for term in pp:
                    if term.isdigit:
                        continue
                    else:
                        violate = True
                        break
                if not violate:
                    self.ip_ = ".".join(pp)
                    break

        # 正文处理
        self.trunk = trunk
```

​		过程中遇到的主要问题包括

- 某封邮件没有特定条目
- Subject 用 MIME 编码后如果有多行，行首没有缩进，导致判断失误
- 错误截断 MIME 编码，导致 decode_header 返回的是 str 而非 bytes，从而报错 AttributeError 或 ValueError
- 某些声明 GB2312 编码的段落实际使用的是 GBK，导致报错 UnicodeDecodeError

完成修改后，能顺利读完所有邮件，并正确解析大部分邮件头的信息。

### 2.3 邮件正文分词

​		采用孙茂松等实现的分词工具包 THULAC 进行分词。为了减少模型载入的时间，采用一次初始化的办法。因为 THULAC 的分词结果可以区分词性，我们选择以下处理方式：

```
class Mail:
		def __init(self, label, address, header, trunk, lac):
				# 以上邮件头部分省略
				# 正文处理
        self.title_split = [entry[0] for entry in lac.cut(self.subject_) if not entry[1].startswith('w')]
        trunk_trim = trunk.replace(" ", "").replace("\n", "").replace("\u3000", "")
        word_split = lac.cut(trunk_trim)
        self.nouns = [entry[0] for entry in word_split if entry[1].startswith('n')]
        self.verbs = [entry[0] for entry in word_split if entry[1].startswith('v')]
        self.adjvs = [entry[0] for entry in word_split if entry[1].startswith(('a', 'd'))]
        other_suffixes = ('t', 'f', 's', 'h', 'k', 'i', 'j', 'x')
        ignored = ('m', 'q', 'r', 'c', 'p', 'u', 'y', 'e', 'o', 'g', 'w')  # 一些虚词, 连词, 叹词...
        self.other = [entry[0] for entry in word_split if entry[1].startswith(other_suffixes)]
        self.ignored = [entry[0] for entry in word_split if entry[1].startswith(ignored)]
```

这里把各词按词性稍微组织了一下，分成了五个类，分别是名词、动词、形容词&副词、其他意义不太明确的词和没有实际意义的词。希望这个方法可以排除部分干扰。

​		这样按照上述循环就可以依次把每封邮件的头部、正文和标签都处理完成：

```python
import jsonpickle
import thulac  # pip 安装版本需要把 thulac/character/CBTaggingDecoder.py 第 170 行中 time.clock() 改成 time.time()
import Mail

with open("trec06c-utf8/label/index", mode='rt') as f:
    input_index = f.read()

indexes = input_index.split('\n')
lac = thulac.thulac()

mail_data = []
for index in indexes[:-1]:  # 最后有一个空行
    label, path = index.split(" ")
    address = path[7:]  # 去掉 ../data
    with open("trec06c-utf8/data" + address, mode='rt') as f:
        uncut = f.read()
    parts = uncut.split('\n\n')
    head = parts[0]
    trunk = '\n\n'.join(parts[1:])
    lines = head.split('\n')
    entry = ""
    header = {}
    for line in lines:  # 拆文件头
        if line.startswith("\t") or line.startswith('    '):  # 继续上一条 entry
            header[entry] += "\n" + line.lstrip()
        else:  # 新 entry
            pars = line.split(": ")  # 防止时间遭拆分
            entry = pars[0]
            header[entry] = ': '.join(pars[1:])
    this_mail = Mail(label, address, header, trunk, lac)
    mail_data.append(this_mail)
```

这个分词的过程很花时间（跑完一共六个多小时），因此将成果存下来：

```python
frozen = jsonpickle.encode(mail_data)
f = open("splited.json", mode='wt')
f.write(frozen)
f.close()
```

这个 splited.json 文件比较大，就不上传了。



# 三、朴素贝叶斯分类器的实现

### 3.1 基础模型

​		提取所有已分词文本，建立词汇表，然后提取特征：

```python
from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer, HashingVectorizer # 提取文本特征向量的类
from sklearn.naive_bayes import MultinomialNB, BernoulliNB, ComplementNB # 三种朴素贝叶斯算法，差别在于估计p(x|y)的方式

texts = []
labels = []
for mail in mail_data:
    text = " ".join(mail.nouns) + " ".join(mail.verbs) + " ".join(mail.adjvs) + " ".join(mail.other)
    label = 1 if mail.label == 'spam' else 0
    texts.append(text)
    labels.append(label)

vectorizer = TfidfVectorizer()
vectorizer.fit(texts)
vectors = vectorizer.transform(texts)
```

这里对 texts 一起提取是更为合适的，它会将所有样本提取为一个稀疏矩阵，如果分开提取则反而不好处理了。

​		然后划分训练集和测试集

```python
x_train, x_test, y_train, y_test = train_test_split(vectors, labels, test_size=0.2)
```

事实证明对稀疏矩阵这个函数也可以用，而且划分完得到的还是稀疏矩阵。

​		接下来实现一个准确率计算函数

```python
def acc(y, y_hat):
    true_ = p_true = p_real = p_hat = 0.
    for y_i, y_i_hat in zip(y, y_hat):
        if y_i == 1:  # 计数 TP+FP
            p_real += 1
        if y_i_hat == 1:  # 计数 TP+FN
            p_hat += 1
        if y_i == y_i_hat:
            true_ += 1
            if y_i == 1:  # 计数 TP
                p_true += 1
    accuracy = true_ / len(y)
    precision = p_true / p_real
    recall = p_true / p_hat
    return accuracy, precision, recall
```

原理也很清晰，唯一值得疑问的是，因为取 spam 为 1，这三个参数并没有反映出代价较大的情况 “正常邮件被识别为病毒邮件”，因此基于它，我又实现了一个计算以 ham 为 1 的三个参数的函数，记为 acc2：

```python
def acc2(y, y_hat):
    true_ = p_true = p_real = p_hat = 0.
    for y_i, y_i_hat in zip(y, y_hat):
        if y_i == 0:  # 计数 TP+FP
            p_real += 1
        if y_i_hat == 0:  # 计数 TP+FN
            p_hat += 1
        if y_i == y_i_hat:
            true_ += 1
            if y_i == 0:  # 计数 TP
                p_true += 1
    accuracy = true_ / len(y)
    precision = p_true / p_real
    recall = p_true / p_hat
    return accuracy, precision, recall
```

用这两个函数来汇报实验结果。

​		使用三种朴素贝叶斯的分类器分别进行计算：

```python
clf = MultinomialNB()
clf.fit(x_train, y_train)
y_predict = clf.predict(x_test)

clf2 = BernoulliNB()
clf2.fit(x_train, y_train)
y_predict2 = clf2.predict(x_test)

clf3 = ComplementNB()
clf3.fit(x_train, y_train)
y_predict3 = clf3.predict(x_test)
```

​		最后的结果可以制成表一同报告：

```python
print("|   Model_Name  |  Accuracy  |  Precision |   Recall   | ~Precision |   ~Recall  |")
print("---------------------------------------------------------------------------------")
s_format = "|%10.4f  |%10.4f  |%10.4f  |%10.4f  |%10.4f  |"
print("|%14s" % "MultinomialNB", s_format % accu1)
print("|%14s" % "BernoulliNB", s_format % accu2)
print("|%14s" % "ComplementNB", s_format % accu3)
```

其中 Precision 和 Recall 是以 spam 为正例汇报的，而 \~Precision 和 \~Recall 是以 ham 为正例汇报的。该基础模型的结果如下：

```
|   Model_Name  |  Accuracy  |  Precision |   Recall   | ~Precision |   ~Recall  |
---------------------------------------------------------------------------------
| MultinomialNB |    0.9769  |    0.9936  |    0.9723  |    0.9433  |    0.9866  |
|   BernoulliNB |    0.9205  |    0.8941  |    0.9854  |    0.9735  |    0.8211  |
|  ComplementNB |    0.9754  |    0.9828  |    0.9803  |    0.9605  |    0.9654  |
```

可以看到 MultinomialNB 和 ComplementNB 的结果比较好，且 ComplementNB 表现较为均衡，MultinomialNB 在特定任务上，尤其是代表没被拦截的正常邮件的 ~Recall 指标更优。这些准确率都达到了要求的 90%。



### 3.2 模型比较

#### 3.2.1 词表大小

​		因为我确实没有想到该以怎样的指标筛选去掉部分单词，考虑用分词模型划分的词类来筛选。因此，我们只需改动

```python
text = " ".join(mail.nouns) + " ".join(mail.verbs) + " ".join(mail.adjvs) + " ".join(mail.other)
```

这一行即可改变我们使用的词表。以下为结论：

```
text = " ".join(mail.nouns)

|   Model_Name  |  Accuracy  |  Precision |   Recall   | ~Precision |   ~Recall  |
---------------------------------------------------------------------------------
| MultinomialNB |    0.9732  |    0.9903  |    0.9702  |    0.9387  |    0.9796  |
|   BernoulliNB |    0.8948  |    0.8526  |    0.9883  |    0.9797  |    0.7675  |
|  ComplementNB |    0.9624  |    0.9622  |    0.9811  |    0.9627  |    0.9268  |
```

```
text = " ".join(mail.verbs)

|   Model_Name  |  Accuracy  |  Precision |   Recall   | ~Precision |   ~Recall  |
---------------------------------------------------------------------------------
| MultinomialNB |    0.9662  |    0.9869  |    0.9632  |    0.9250  |    0.9725  |
|   BernoulliNB |    0.8950  |    0.8645  |    0.9750  |    0.9558  |    0.7799  |
|  ComplementNB |    0.9258  |    0.9126  |    0.9743  |    0.9521  |    0.8455  |
```

```
text = " ".join(mail.adjvs)

|   Model_Name  |  Accuracy  |  Precision |   Recall   | ~Precision |   ~Recall  |
---------------------------------------------------------------------------------
| MultinomialNB |    0.9405  |    0.9727  |    0.9403  |    0.8757  |    0.9409  |
|   BernoulliNB |    0.9258  |    0.9686  |    0.9240  |    0.8396  |    0.9300  |
|  ComplementNB |    0.8740  |    0.8442  |    0.9626  |    0.9340  |    0.7486  |
```

```
text = " ".join(mail.other)

|   Model_Name  |  Accuracy  |  Precision |   Recall   | ~Precision |   ~Recall  |
---------------------------------------------------------------------------------
| MultinomialNB |    0.9632  |    0.9905  |    0.9558  |    0.9087  |    0.9795  |
|   BernoulliNB |    0.9668  |    0.9884  |    0.9628  |    0.9238  |    0.9755  |
|  ComplementNB |    0.9627  |    0.9606  |    0.9830  |    0.9669  |    0.9249  |
```

```
text = " ".join(mail.ignored)

|   Model_Name  |  Accuracy  |  Precision |   Recall   | ~Precision |   ~Recall  |
---------------------------------------------------------------------------------
| MultinomialNB |    0.9360  |    0.9635  |    0.9412  |    0.8821  |    0.9251  |
|   BernoulliNB |    0.9262  |    0.9399  |    0.9481  |    0.8993  |    0.8843  |
|  ComplementNB |    0.8962  |    0.8755  |    0.9643  |    0.9366  |    0.7934  |
```

从上述结论中可以看出，没有采用 ignored 果然是正确的，形容词的分类效果最差，而其他三个都差不多，且使用一类词的效果不如之前。因此，我们将 adjvs 去除，得到

```
text = " ".join(mail.nouns) + " ".join(mail.verbs) + " ".join(mail.other)

|   Model_Name  |  Accuracy  |  Precision |   Recall   | ~Precision |   ~Recall  |
---------------------------------------------------------------------------------
| MultinomialNB |    0.9772  |    0.9946  |    0.9716  |    0.9430  |    0.9890  |
|   BernoulliNB |    0.9199  |    0.8903  |    0.9876  |    0.9780  |    0.8197  |
|  ComplementNB |    0.9754  |    0.9820  |    0.9809  |    0.9624  |    0.9646  |
```

我们可以看到相比原本的结果，准确率有了小幅的提升。



#### 3.2.2 引用邮件头信息

​		因为邮件主题可以分词，而 ip 和发件方域名可以整体作为一个特征加入词表，因此我考虑将其直接加入 text：

```python
text += mail.ip_
if mail.from_add.find("@") != -1:
    text += mail.from_add.split("@")[1]
```

这使得结果准确率有了较大提升：

```
|   Model_Name  |  Accuracy  |  Precision |   Recall   | ~Precision |   ~Recall  |
---------------------------------------------------------------------------------
| MultinomialNB |    0.9829  |    0.9958  |    0.9787  |    0.9578  |    0.9915  |
|   BernoulliNB |    0.9397  |    0.9197  |    0.9883  |    0.9788  |    0.8622  |
|  ComplementNB |    0.9831  |    0.9925  |    0.9820  |    0.9647  |    0.9851  |
```



#### 3.2.3 特征提取方法

​		根据直觉，考虑到词汇出现在其他文档频度而降低其权重的 TfidfVectorizer 会更优，在上述情境下，对比 CountVectorizer 的表现，发现这一直觉也不全成立：

```
vectorizer = CountVectorizer()

|   Model_Name  |  Accuracy  |  Precision |   Recall   | ~Precision |   ~Recall  |
---------------------------------------------------------------------------------
| MultinomialNB |    0.9823  |    0.9845  |    0.9888  |    0.9778  |    0.9695  |
|   BernoulliNB |    0.9455  |    0.9269  |    0.9907  |    0.9826  |    0.8710  |
|  ComplementNB |    0.9820  |    0.9841  |    0.9889  |    0.9780  |    0.9686  |
```

当然这一变动在 ~Recall 上的损失是不可接受的。
