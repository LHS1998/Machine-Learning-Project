# 第七次实验报告

## 实验目标

​	本实验以真实游戏中非结构化的日志数据出发，构建用户流失预测模型，预测测试集中的用户是否为流失用户。         

​	本实验报告中所有代码均为示例代码，为了叙述清晰，部分是从函数、类中粘贴出来，不保证单独可以正确执行。作者所使用的环境是 MacOS 12.2.1，Python 3.9，PyCharm 2021.3.1，能正确运行各脚本。



## 数据处理

​	数据处理部分的脚本入口为 `feature_extraction.py`。

### 数据读入

​	本次实验共五份数据文件，其中三份为标签，一份为用户游玩每个关卡的记录，一份为每个关卡的统计信息。为了方便匹配，设计以下函数用于读取信息。

​	对于 `train.csv` 和 `dev.csv`，需要读入 id 以及类别，设计函数如下：

```python
def read_label(path, sep='\t'):
    labels = {}
    with open(path, mode='rt') as f:
        f.readline()  # 表头
        while True:
            line = f.readline()
            if not line:
                break
            info = line.split(sep)
            labels[int(info[0])] = int(info[1])
    return labels 
```

对于 `test.csv`，只需读入 id，设计函数如下：

```python
def read_name(path):
    with open(path, mode='rt') as f:
        f.readline()  # 表头
        line = f.read()
        line = line.rstrip('\n')  # 去除末尾一行 \n
    return set([int(id_) for id_ in line.split('\n')])
```

对于另外两个数据文件，采用具名元组读入信息，设计读取函数如下：

```python
def read_seq(path, sep='\t', tokenizer=(lambda x: x)):
    """
    提取 csv 信息
    :param path: csv 路径
    :param sep: csv 分隔符
    :param tokenizer: callable，处理行数据，接收以列表传入的一行信息，返回同维度的处理后的信息
    :return: 记录（作为具名元组）的列表
    """
    seqs = []
    with open(path, mode='rt') as f:
        header = f.readline()  # 表头
        Record = namedtuple('Record', header)
        while True:
            line = f.readline()
            if not line:
                break
            line = line.rstrip('\n')
            row = line.split(sep)
            row = tokenizer(row)
            seqs.append(Record(*row))
    return seqs
```

这样可以分别处理两个文件的各列信息。主要的目的是把用户信息里的时间字符串给转换成可以比较的数据格式：

```python
def seq_parser(row):
    """
    提取 './data/level_seq.csv' 中各列信息
    """
    return int(row[0]), int(row[1]), int(row[2]), float(row[3]), float(row[4]), int(row[5]), parser.isoparse(row[6])

def meta_parser(row):
    """
    提取 './data/level_meta.csv' 中各列信息
    """
    return float(row[0]), float(row[1]), float(row[2]), float(row[3]), int(row[4])
```

​	使用上述工具读取信息如下：

```python
train_label = read_label('./data/train.csv')
validate_label = read_label('./data/dev.csv')
test_label = read_name('./data/test.csv')
user_seq = read_seq('./data/level_seq.csv', tokenizer=seq_parser)
level_meta = read_seq('./data/level_meta.csv', tokenizer=meta_parser)
```

### 用户匹配

​	接下来将同一用户的序列信息匹配到一个列表中，并划分训练集、验证集和测试集：

```python
train_seq = {}
validate_seq = {}
test_seq = {}
not_found_seq = set()
for seq in user_seq:
    if seq.user_id in train_label:  # 判断所在集合
        handler = train_seq
    elif seq.user_id in validate_label:
        handler = validate_seq
    elif seq.user_id in test_label:
        handler = test_seq
    else:
        if seq.user_id in not_found_seq:
            continue
        else:
            not_found_seq.add(seq.user_id)
            print("User %d not found in label file." % seq.user_id)
            continue
    handler.setdefault(seq.user_id, []).append(seq)  # 用列表保存同一用户的若干游玩记录
```

### 特征提取

​	通过对数据的观察，设计以下特征维度：

​	第一组是体现个人游玩历史的特征，包括

- `tries`，用户总共的游玩次数，衡量用户的耐心和坚决程度
- `process`，用户目前玩到的最后一关，衡量用户的关卡进度
- `history`，用户在数据中的首个关卡，区分用户是老玩家还是新玩家
- `success_rate`，用户游玩过程中的胜率，衡量用户的能力以及用户受到的劝退程度
- `time_spent`，用户玩游戏的总时间，衡量用户对游戏的投入程度
- `help_used`，用户在关卡中使用提示或道具的总次数，衡量用户对游戏的投入程度
- `time_stamp`，用户在每个小时玩游戏的次数
- `most_played_hour`，用户最常在哪个时段玩游戏，反映用户的年龄、工作、游玩习惯等信息
- `mean_rest_step`，用户通关后剩余的平均步数，也许有用
- `last_win`，用户最后一次游玩是否成功，考虑如果失败可能会导致用户弃坑

​	第二组是体现个人相对群体的游玩水平，包括

- `excess_success_rate`，相比游玩关卡次数加权的关卡平均通关率，用户的额外平均通关率
- `excess_tries`，相比游玩关卡次数加权的关卡平均尝试次数，用户的额外尝试次数
- `excess_time`，相比游玩关卡的加权耗时，用户的额外耗时

用类 `User` 封装提取过程，代码如下：

```python
class User:

    def __init__(self, user_seq, levels, label):
        max_level = max([level.level_id for level in levels])
        # 个人的游玩历史
        self.tries = len(user_seq)  # 游玩次数
        level_process = [seq.level_id for seq in user_seq]  # 游玩的关卡序列
        self.process = max(level_process) / max_level  # 游玩进度
        self.history = min(level_process) / max_level  # 游玩了几关
        self.success_rate = sum([seq.f_success for seq in user_seq]) / self.tries  # 通关比例
        self.time_spent = sum([seq.f_duration for seq in user_seq])  # 总游戏时间
        self.help_used = sum([seq.f_help for seq in user_seq])  # 道具使用总数
        self.time_stamp = [0] * 24  # 统计几点玩游戏
        for seq in user_seq:
            self.time_stamp[seq.time.hour] += 1
        self.most_played_hour = np.argmax(self.time_stamp)  # 常在什么时候玩游戏（具体到小时）
        rest_step = []  # 统计成功时剩余步数
        for seq in user_seq:
            if seq.f_success:
                rest_step.append(seq.f_reststep)
        if rest_step:  # 有一个样例没有赢过，np.mean([]) 返回 NaN，这个后期还不好检查出来，必须做判断
            self.mean_rest_step = np.mean(rest_step)  # 平均通关时剩余步数
        else:
            self.mean_rest_step = 0
        self.last_win = sorted(user_seq, key=(lambda seq: seq.time), reverse=True)[0].f_success
        # assert self.history * self.success_rate == self.tries  # 不一定是每关只通一次

        # 相对的游玩水平
        metadata = [levels[level-1] for level in level_process]  # 所游玩关卡的参数, 保留重复，自动加权
        self.excess_success_rate = self.success_rate - np.mean([meta.f_avg_passrate for meta in metadata])  # 超额胜率
        self.excess_tries = self.tries - sum([meta.f_avg_retrytimes for meta in metadata])  # 超额游玩次数
        time_baseline = 0
        for seq, meta in zip(user_seq, metadata):  # 计算基准的总游玩时间
            if seq.f_success:
                time_baseline += meta.f_avg_win_duration
            else:
                time_baseline += meta.f_avg_duration
        self.excess_time = self.time_spent - time_baseline  # 超额游玩时间
        self.label = label

    def __repr__(self):
        features = [self.tries, "%.5f" % self.process, "%.5f" % self.history, "%.5f" % self.success_rate,
                    self.time_spent, self.help_used, *self.time_stamp, self.most_played_hour,
                    "%.5f" % self.mean_rest_step, self.last_win, "%.5f" % self.excess_success_rate,
                    "%.5f" % self.excess_tries, "%.5f" % self.excess_time]
        features = [str(item) for item in features]
        return ", ".join(features) + '\n'
```

将结果写入 csv 文件，代码如下：

```python
CSV_HEADER = 'tries, process, history, success_rate, time_spent, help_used, '
for i in range(24):
    CSV_HEADER += 'time_stamp_%d, ' % i
CSV_HEADER += 'most_played_hour, mean_rest_step, last_win, excess_success_rate, excess_tries, excess_time\n'

def to_csv(path, objects):
    path = path.split('.')[0]
    with open(path + '_data.csv', mode='wt') as f:
        f.write(CSV_HEADER)
        for obj in objects:
            f.write(repr(obj))
    with open(path + '_label.csv', mode='wt') as f:
        f.write("label\n")
        for obj in objects:
            f.write(str(obj.label) + "\n")
```

接着对三个数据集分别处理即可：

```python
train_features = []
for user in train_seq:
    train_features.append(User(train_seq[user], level_meta, train_label[user]))
to_csv("data_processed/train.csv", train_features)

validate_features = []
for user in validate_seq:
    validate_features.append(User(validate_seq[user], level_meta, validate_label[user]))
to_csv("data_processed/validate.csv", validate_features)

test_features = []
for user in test_seq:
    test_features.append(User(test_seq[user], level_meta, user))  # 测试集以用户 id 作为标签，便于输出结果
to_csv("data_processed/test.csv", test_features)

all_for_test = train_features + validate_features
to_csv("data_processed/all_train.csv", all_for_test)
```



## 分类器的选择

​	验证集上测试的部分脚本入口为 `hw7_dev.py`。

### 代码框架

​	用以下框架筛选基分类器。使用验证集：

```python
train_data = pd.read_csv("./data_processed/train_data.csv", sep=',')
train_label = pd.read_csv("./data_processed/train_label.csv", sep=',')
validate_data = pd.read_csv("./data_processed/validate_data.csv", sep=',')
validate_label = pd.read_csv("./data_processed/validate_label.csv", sep=',')
```

​	尝试几下三种预处理：

```python
from sklearn.preprocessing import MinMaxScaler, StandardScaler, Normalizer

scaler = MinMaxScaler()
train_minmax = scaler.fit_transform(train_data)
validate_minmax = scaler.transform(validate_data)

scaler = StandardScaler()
train_std = scaler.fit_transform(train_data)
validate_std = scaler.transform(validate_data)

scaler = Normalizer()
train_norm = scaler.fit_transform(train_data)
validate_norm = scaler.transform(validate_data)
```

​	单个测试和记录的框架：

```python
logger = open("single_model_result.txt", mode='at')
logger.write("====> [%s] <====\n" % time.strftime('%Y-%m-%d %H:%M:%S', time.localtime(time.time())))
log_template = "分类器：%s. AUC = %.7f\n"

def single_test(classifier, x_train, y_train, x_test, y_test, accuracy, precondition, **kwargs):
    clf = classifier(**kwargs)
    clf.fit(x_train, y_train)
    y_hat = clf.predict(x_test)
    auc = accuracy(y_test, y_hat)
    logger.write(log_template % ('%r@%r@%s' % (classifier, kwargs, precondition), auc))
    return y_hat
```

​	如下为一次测试的代码：

```python
single_test(DecisionTreeClassifier, train_data.values, train_label.values, validate_data.values, validate_label.values,
            roc_auc_score, 'None', max_depth=10, min_samples_split=10)
```

可以调参：

```python
for thres in [0.6, 0.7, 0.8, 0.9]:
    mean_test(10, thres, DecisionTreeClassifier, train_norm, train_label.values, validate_norm, validate_label.values,
              roc_auc_score, 'None', max_depth=10, min_samples_split=10)
```

可以批量测试（无参数）多种分类器：

```python
for clf in [KNN, LinearSVC, AdaBoostClassifier, RandomForestClassifier, GradientBoostingClassifier, IsolationForest]:
    single_test(clf, train_data.values, train_label.values, validate_data.values, validate_label.values,
                roc_auc_score, 'None')
```

可以批量测试不同参数：

```python
for args in [{"base_estimator": DecisionTreeClassifier(max_depth=5), "n_estimators": 1000, "max_samples": 0.3},
             {"base_estimator": DecisionTreeClassifier(max_depth=1), "n_estimators": 1000, "max_samples": 0.3},
             {"base_estimator": SVC(), "n_estimators": 1000, "max_samples": 0.3},
             {"base_estimator": KNN(n_neighbors=2), "n_estimators": 1000, "max_samples": 0.3},
             {"base_estimator": KNN(n_neighbors=3), "n_estimators": 1000, "max_samples": 0.3}]:
    single_test(ensemble.BaggingClassifier, train_data.values, train_label.values,
                validate_data.values, validate_label.values,
                roc_auc_score, 'None', **args)
```

输出结果如下。

```python
====> [2022-05-22 21:56:18] <====
分类器：<class 'sklearn.tree._classes.DecisionTreeClassifier'>@{'max_depth': 10, 'min_samples_split': 10}@None. AUC = 0.6527961
分类器：<class 'sklearn.tree._classes.DecisionTreeClassifier'>@{'max_depth': 10, 'min_samples_split': 10}@MinMax. AUC = 0.6497227
分类器：<class 'sklearn.tree._classes.DecisionTreeClassifier'>@{'max_depth': 10, 'min_samples_split': 10}@Standard. AUC = 0.6530665
分类器：<class 'sklearn.tree._classes.DecisionTreeClassifier'>@{'max_depth': 10, 'min_samples_split': 10}@Normalizer. AUC = 0.6234409

====> [2022-05-23 21:09:12] <====
分类器：<class 'sklearn.neighbors._classification.KNeighborsClassifier'>@{}@None. AUC = 0.6144908
分类器：<class 'sklearn.neighbors._classification.KNeighborsClassifier'>@{}@MinMax. AUC = 0.6169809
分类器：<class 'sklearn.neighbors._classification.KNeighborsClassifier'>@{}@Standard. AUC = 0.6228721
分类器：<class 'sklearn.neighbors._classification.KNeighborsClassifier'>@{}@Normalizer. AUC = 0.5884794
分类器：<class 'sklearn.svm._classes.LinearSVC'>@{}@None. AUC = 0.5143005
分类器：<class 'sklearn.svm._classes.LinearSVC'>@{}@MinMax. AUC = 0.6428486
分类器：<class 'sklearn.svm._classes.LinearSVC'>@{}@Standard. AUC = 0.6475159
分类器：<class 'sklearn.svm._classes.LinearSVC'>@{}@Normalizer. AUC = 0.5724418

====> [2022-05-24 19:50:52] <====
分类器：<class 'sklearn.tree._classes.DecisionTreeClassifier'>@{'max_depth': 10, 'min_samples_split': 10}@None. AUC = 0.6524974
分类器：<class 'sklearn.tree._classes.DecisionTreeClassifier'>@{'max_depth': 5, 'min_samples_split': 10}@None. AUC = 0.6518562
分类器：<class 'sklearn.tree._classes.DecisionTreeClassifier'>@{'max_depth': 20, 'min_samples_split': 10}@None. AUC = 0.5885379
分类器：<class 'sklearn.tree._classes.DecisionTreeClassifier'>@{'min_samples_split': 10}@None. AUC = 0.5799292

====> [2022-05-24 19:52:12] <====
分类器：<class 'sklearn.tree._classes.DecisionTreeClassifier'>@{'max_depth': 6, 'min_samples_split': 10}@None. AUC = 0.6560111
分类器：<class 'sklearn.tree._classes.DecisionTreeClassifier'>@{'max_depth': 7, 'min_samples_split': 10}@None. AUC = 0.6492097
分类器：<class 'sklearn.tree._classes.DecisionTreeClassifier'>@{'max_depth': 8, 'min_samples_split': 10}@None. AUC = 0.6482281
分类器：<class 'sklearn.tree._classes.DecisionTreeClassifier'>@{'max_depth': 9, 'min_samples_split': 10}@None. AUC = 0.6574346

====> [2022-05-24 19:52:37] <====
分类器：<class 'sklearn.tree._classes.DecisionTreeClassifier'>@{'max_depth': 4, 'min_samples_split': 10}@None. AUC = 0.6589706
分类器：<class 'sklearn.tree._classes.DecisionTreeClassifier'>@{'max_depth': 3, 'min_samples_split': 10}@None. AUC = 0.6661279
分类器：<class 'sklearn.tree._classes.DecisionTreeClassifier'>@{'max_depth': 2, 'min_samples_split': 10}@None. AUC = 0.6141905
分类器：<class 'sklearn.tree._classes.DecisionTreeClassifier'>@{'max_depth': 1, 'min_samples_split': 10}@None. AUC = 0.6883558

====> [2022-05-24 20:00:36] <====
分类器：<class 'sklearn.tree._classes.DecisionTreeClassifier'>@{'max_depth': 5}@None. AUC = 0.6518562
分类器：<class 'sklearn.tree._classes.DecisionTreeClassifier'>@{'max_depth': 5, 'min_samples_split': 2}@None. AUC = 0.6518704
分类器：<class 'sklearn.tree._classes.DecisionTreeClassifier'>@{'max_depth': 5, 'min_samples_split': 5}@None. AUC = 0.6518704
分类器：<class 'sklearn.tree._classes.DecisionTreeClassifier'>@{'max_depth': 5, 'min_samples_split': 10}@None. AUC = 0.6518562
分类器：<class 'sklearn.tree._classes.DecisionTreeClassifier'>@{'max_depth': 5, 'min_samples_split': 20}@None. AUC = 0.6515858
分类器：<class 'sklearn.tree._classes.DecisionTreeClassifier'>@{'max_depth': 5, 'min_samples_split': 30}@None. AUC = 0.6515858
分类器：<class 'sklearn.tree._classes.DecisionTreeClassifier'>@{'max_depth': 5, 'min_samples_split': 50}@None. AUC = 0.6515858
分类器：<class 'sklearn.tree._classes.DecisionTreeClassifier'>@{'max_depth': 5, 'min_samples_split': 100}@None. AUC = 0.6513155

====> [2022-05-24 20:30:07] <====
分类器：<class 'sklearn.ensemble._weight_boosting.AdaBoostClassifier'>@{}@None. AUC = 0.6673089
分类器：<class 'sklearn.ensemble._weight_boosting.AdaBoostClassifier'>@{}@MinMax. AUC = 0.6673089
分类器：<class 'sklearn.ensemble._weight_boosting.AdaBoostClassifier'>@{}@Standard. AUC = 0.6673089
分类器：<class 'sklearn.ensemble._weight_boosting.AdaBoostClassifier'>@{}@Normalizer. AUC = 0.6520694
分类器：<class 'sklearn.ensemble._forest.RandomForestClassifier'>@{}@None. AUC = 0.6413547
分类器：<class 'sklearn.ensemble._forest.RandomForestClassifier'>@{}@MinMax. AUC = 0.6512441
分类器：<class 'sklearn.ensemble._forest.RandomForestClassifier'>@{}@Standard. AUC = 0.6454528
分类器：<class 'sklearn.ensemble._forest.RandomForestClassifier'>@{}@Normalizer. AUC = 0.6404299
分类器：<class 'sklearn.ensemble._gb.GradientBoostingClassifier'>@{}@None. AUC = 0.6629547
分类器：<class 'sklearn.ensemble._gb.GradientBoostingClassifier'>@{}@MinMax. AUC = 0.6632108
分类器：<class 'sklearn.ensemble._gb.GradientBoostingClassifier'>@{}@Standard. AUC = 0.6626843
分类器：<class 'sklearn.ensemble._gb.GradientBoostingClassifier'>@{}@Normalizer. AUC = 0.6582590
分类器：<class 'sklearn.ensemble._iforest.IsolationForest'>@{}@None. AUC = 0.5367384
分类器：<class 'sklearn.ensemble._iforest.IsolationForest'>@{}@MinMax. AUC = 0.5445357
分类器：<class 'sklearn.ensemble._iforest.IsolationForest'>@{}@Standard. AUC = 0.5367668
分类器：<class 'sklearn.ensemble._iforest.IsolationForest'>@{}@Normalizer. AUC = 0.4914766

====> [2022-05-25 15:25:28] <====
分类器：<class 'sklearn.ensemble._bagging.BaggingClassifier'>@{'base_estimator': DecisionTreeClassifier(max_depth=5), 'n_estimators': 1000, 'max_samples': 0.3}@None. AUC = 0.6588708
分类器：<class 'sklearn.ensemble._bagging.BaggingClassifier'>@{'base_estimator': DecisionTreeClassifier(max_depth=5), 'n_estimators': 1000, 'max_samples': 0.3}@MinMax. AUC = 0.6597103
分类器：<class 'sklearn.ensemble._bagging.BaggingClassifier'>@{'base_estimator': DecisionTreeClassifier(max_depth=5), 'n_estimators': 1000, 'max_samples': 0.3}@Standard. AUC = 0.6622146
分类器：<class 'sklearn.ensemble._bagging.BaggingClassifier'>@{'base_estimator': DecisionTreeClassifier(max_depth=5), 'n_estimators': 1000, 'max_samples': 0.3}@Normalizer. AUC = 0.6563380
分类器：<class 'sklearn.ensemble._bagging.BaggingClassifier'>@{'base_estimator': DecisionTreeClassifier(max_depth=1), 'n_estimators': 1000, 'max_samples': 0.3}@None. AUC = 0.6774977
分类器：<class 'sklearn.ensemble._bagging.BaggingClassifier'>@{'base_estimator': DecisionTreeClassifier(max_depth=1), 'n_estimators': 1000, 'max_samples': 0.3}@MinMax. AUC = 0.6796322
分类器：<class 'sklearn.ensemble._bagging.BaggingClassifier'>@{'base_estimator': DecisionTreeClassifier(max_depth=1), 'n_estimators': 1000, 'max_samples': 0.3}@Standard. AUC = 0.6777397
分类器：<class 'sklearn.ensemble._bagging.BaggingClassifier'>@{'base_estimator': DecisionTreeClassifier(max_depth=1), 'n_estimators': 1000, 'max_samples': 0.3}@Normalizer. AUC = 0.6767151
分类器：<class 'sklearn.ensemble._bagging.BaggingClassifier'>@{'base_estimator': SVC(), 'n_estimators': 1000, 'max_samples': 0.3}@None. AUC = 0.6801019
分类器：<class 'sklearn.ensemble._bagging.BaggingClassifier'>@{'base_estimator': SVC(), 'n_estimators': 1000, 'max_samples': 0.3}@MinMax. AUC = 0.5903139
分类器：<class 'sklearn.ensemble._bagging.BaggingClassifier'>@{'base_estimator': SVC(), 'n_estimators': 1000, 'max_samples': 0.3}@Standard. AUC = 0.6526527
分类器：<class 'sklearn.ensemble._bagging.BaggingClassifier'>@{'base_estimator': SVC(), 'n_estimators': 1000, 'max_samples': 0.3}@Normalizer. AUC = 0.5257979
分类器：<class 'sklearn.ensemble._bagging.BaggingClassifier'>@{'base_estimator': KNeighborsClassifier(n_neighbors=2), 'n_estimators': 1000, 'max_samples': 0.3}@None. AUC = 0.6264288
分类器：<class 'sklearn.ensemble._bagging.BaggingClassifier'>@{'base_estimator': KNeighborsClassifier(n_neighbors=2), 'n_estimators': 1000, 'max_samples': 0.3}@MinMax. AUC = 0.6374136
分类器：<class 'sklearn.ensemble._bagging.BaggingClassifier'>@{'base_estimator': KNeighborsClassifier(n_neighbors=2), 'n_estimators': 1000, 'max_samples': 0.3}@Standard. AUC = 0.6392780
分类器：<class 'sklearn.ensemble._bagging.BaggingClassifier'>@{'base_estimator': KNeighborsClassifier(n_neighbors=2), 'n_estimators': 1000, 'max_samples': 0.3}@Normalizer. AUC = 0.6088549
分类器：<class 'sklearn.ensemble._bagging.BaggingClassifier'>@{'base_estimator': KNeighborsClassifier(n_neighbors=3), 'n_estimators': 1000, 'max_samples': 0.3}@None. AUC = 0.6362468
分类器：<class 'sklearn.ensemble._bagging.BaggingClassifier'>@{'base_estimator': KNeighborsClassifier(n_neighbors=3), 'n_estimators': 1000, 'max_samples': 0.3}@MinMax. AUC = 0.6413550
分类器：<class 'sklearn.ensemble._bagging.BaggingClassifier'>@{'base_estimator': KNeighborsClassifier(n_neighbors=3), 'n_estimators': 1000, 'max_samples': 0.3}@Standard. AUC = 0.6419102
分类器：<class 'sklearn.ensemble._bagging.BaggingClassifier'>@{'base_estimator': KNeighborsClassifier(n_neighbors=3), 'n_estimators': 1000, 'max_samples': 0.3}@Normalizer. AUC = 0.6000896
```

直观来看还是决策树和 SVC 作为基处理器的效果最好。



## 提交数据的生成

​	测试集上数据处理部分的脚本入口为 `hw7.py`。

​	采用训练集和验证集为模型进行训练。代码如下：

```python
import pandas as pd
from sklearn.tree import DecisionTreeClassifier
from sklearn.preprocessing import StandardScaler
from sklearn.svm import SVC
from sklearn.pipeline import make_pipeline
from sklearn import ensemble
from Bagging import Bagging

train_data = pd.read_csv("./data_processed/all_train_data.csv", sep=',')
train_label = pd.read_csv("./data_processed/all_train_label.csv", sep=',')
test_data = pd.read_csv("./data_processed/test_data.csv", sep=',')
test_label = pd.read_csv("./data_processed/test_label.csv", sep=',')

# 特征选择
drop = [' time_stamp_%d' % i for i in range(23)]
drop += [" mean_rest_step", " most_played_hour", " excess_success_rate", " excess_tries", " excess_time"]
train_data.drop(columns=drop)
test_data.drop(columns=drop)

# 预处理
scaler = StandardScaler()
train_norm = scaler.fit_transform(train_data)
test_norm = scaler.transform(test_data)

# 模型训练
clf = ensemble.RandomForestClassifier(max_depth=1, n_estimators=10000)  # 修改这个

# 使用未经预处理的数据
# clf.fit(train_data.values, train_label.values)
# y_hat = clf.predict_proba(test_norm)

# with open("result.csv", mode='wt') as f:
#     f.write("user_id,proba\n")
#     for y, i in zip(y_hat, test_label.values):
#         f.write("%d, %.5f\n" % (i[0], y))
# y_hat = clf.predict_proba(test_data.values)

# 使用经预处理的数据
clf.fit(train_norm, train_label.values)
y_hat = clf.predict_proba(test_norm)

with open("result.csv", mode='wt') as f:
    f.write("user_id,proba\n")
    for y, i in zip(y_hat[:, 1], test_label.values):
        f.write("%d, %.5f\n" % (i[0], y))
```

对于概率的预测结果准确度和测试数据的数值差异很大，在此处对模型进行进一步调参。经试验，主要的结果如下：

```python
clf = ensemble.BaggingClassifier(DecisionTreeClassifier(max_depth=5), 160)  # 0.74229
clf = ensemble.RandomForestClassifier(max_depth=5, n_estimators=10000)  # 0.74421
clf = ensemble.AdaBoostClassifier(base_estimator=DecisionTreeClassifier(max_depth=5))  # 0.66192
clf = ensemble.StackingClassifier(estimators=[('rf', ensemble.RandomForestClassifier(max_depth=5)),
                                              ('ada', ensemble.AdaBoostClassifier()),
                                              ('dt', DecisionTreeClassifier(max_depth=1))])  # 0.74373
clf = ensemble.StackingClassifier(estimators=[('rf', ensemble.RandomForestClassifier(max_depth=5)),
                                              ('svr', make_pipeline(StandardScaler(), LinearSVC(random_state=42))),
                                              ('dt', DecisionTreeClassifier(max_depth=1))])  # 0.74382
clf = ensemble.StackingClassifier(estimators=[('rf', ensemble.RandomForestClassifier(max_depth=5)),
                                              ('svc', ensemble.BaggingClassifier(SVC(), 30))])  # 0.74391
clf = ensemble.BaggingClassifier(SVC(), 50)  # 全是0-1, 没提交
clf = Bagging(SVC, 100, 0.3)  # hw6 实现的 Bagging 算法, 0.68793
clf = ensemble.BaggingClassifier(DecisionTreeClassifier(max_depth=2), max_samples=0.3, bootstrap_features=True,
                                 n_estimators=1000)  # 0.74196
clf = ensemble.BaggingClassifier(DecisionTreeClassifier(max_depth=1), max_samples=0.3,
                                 n_estimators=1000)  # 0.72963
```

其中备注中是测试集汇报的 AUC。特征选择能小幅提高准确率。预处理小幅降低了准确率。最终选取的最优算法为 `RandomForest`，准确率为 0.74421。
