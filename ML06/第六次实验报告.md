# ç¬¬å…­æ¬¡å®éªŒæŠ¥å‘Š

**æç€šç¬™ 1338875**      

[TOC]

## å®éªŒç›®æ ‡

â€‹		æœ¬å®éªŒä»¥ Amazon ç°å®åœºæ™¯ä¸­çš„è¯„è®ºä¸ºæ•°æ®ï¼Œè®¾è®¡é›†æˆå­¦ä¹ ç®—æ³•é¢„æµ‹è¯„è®ºè´¨é‡ã€‚

â€‹		æœ¬å®éªŒæŠ¥å‘Šä¸­æ‰€æœ‰ä»£ç å‡ä¸ºç¤ºä¾‹ä»£ç ï¼Œä¸ºäº†å™è¿°æ¸…æ™°ï¼Œéƒ¨åˆ†æ˜¯ä»å‡½æ•°ã€ç±»ä¸­ç²˜è´´å‡ºæ¥ï¼Œä¸ä¿è¯å•ç‹¬å¯ä»¥æ­£ç¡®æ‰§è¡Œã€‚ä½œè€…æ‰€ä½¿ç”¨çš„ç¯å¢ƒæ˜¯ MacOS 12.2.1ï¼ŒPython 3.9ï¼ŒPyCharm 2021.3.1ï¼Œèƒ½æ­£ç¡®è¿è¡Œå„è„šæœ¬ã€‚



## æ•°æ®å¤„ç†

### åˆ†è¯

â€‹		é‡‡ç”¨å®éªŒäº”è®²è§£ä¸­çš„åˆ†è¯æ–¹æ³•ã€‚

```python
def word_tokenizer(string):  # ä¸€èˆ¬æ–‡æœ¬åˆ—åˆ†è¯
    words = nltk.tokenize.word_tokenize(string)  # å…ˆåˆ‡è¯
    # stemmer = nltk.stem.SnowballStemmer('english') # ä½¿ç”¨nltkè¯å¹²åŒ–å·¥å…·
    # words = [stemmer.stem(w) for w in words] # å¯¹æ¯ä¸ªè¯è¯å¹²åŒ–
    lemma = nltk.wordnet.WordNetLemmatizer()
    words = [lemma.lemmatize(w).lower() for w in words]
    stopwords = nltk.corpus.stopwords.words('english')  # ä½¿ç”¨nltkçš„åœç”¨è¯
    words = [w for w in words if w not in stopwords]  # å»é™¤åœç”¨è¯
    return words

docs = train_df["reviewText"].tolist()  # å–å‡ºæ‰€æœ‰æ–‡æœ¬å­—ç¬¦ä¸²
docy = test_df["reviewText"].tolist()
vectorizer = TfidfVectorizer(tokenizer=word_tokenizer)
train_x = vectorizer.fit_transform(docs)  # æ–‡æœ¬å‘é‡
train_x = hstack((train_x, train_df[["reviewerID", "asin", "overall"]])).tocsr()  # åŠ å…¥å…¶ä»–ç‰¹å¾
# æµ‹è¯•ç‰¹å¾ä½¿ç”¨è®­ç»ƒé›†ä¸Šç”Ÿæˆçš„è¯­æ–™åº“ï¼Œè¿™æ ·ç±»ä¼¼çœŸå®ç¯å¢ƒï¼Œè€Œä¸”è®­ç»ƒé›†æ²¡æœ‰å‡ºç°çš„è¯å¯¹æµ‹è¯•é›†ä¹Ÿæ²¡æœ‰å¸®åŠ©
test_x = vectorizer.transform(docy)  
# åŠ å…¥å…¶ä»–ç‰¹å¾
test_x = hstack((test_x, test_df[["reviewerID", "asin", "overall"]])).tocsr()  
```

åˆ†è¯çš„è¿‡ç¨‹æ˜¯æ¯”è¾ƒè€—æ—¶çš„ï¼Œå› æ­¤å°†åˆ†è¯ç»“æœå‚¨å­˜ä»¥ä¾›åç»­ä½¿ç”¨ï¼š

```python
np.savez("train_x.npz", data=train_x.data, indices=train_x.indices,
         indptr=train_x.indptr, shape=train_x.shape)
np.savez("test_x.npz", data=test_x.data, indices=test_x.indices,
         indptr=test_x.indptr, shape=test_x.shape)
```

### æ ‡ç­¾çš„ä½¿ç”¨

â€‹		å¯¹äºæ ‡ç­¾æ•°æ®ï¼Œé‰´äºä½œä¸šè¦æ±‚å¯¹æŸæ¡è¯„è®º â€œå¯ä¿¡çš„æ¦‚ç‡â€ ä½œå‡ºä¼°è®¡ï¼Œæˆ‘è€ƒè™‘ä»¥ `votes_up/votes_all` ä½œä¸ºæ ‡ç­¾ä½¿ç”¨ï¼Œå³å–

```python
train_y = train_df["votes_up"] / train_df["votes_all"]
```

åœ¨è®­ç»ƒçš„æ—¶å€™ï¼Œç”Ÿæˆä»¥ä¸‹äºŒå…ƒæ ‡ç­¾ä¾›å•ä¸ªåˆ†ç±»å™¨ä½¿ç”¨ï¼š

```python
[1 if y >= 0.9 else 0 for y in train_y]
```

åœ¨è¯„ä¼°çš„æ—¶å€™ï¼ŒæŠŠå¤šä¸ªåˆ†ç±»å™¨è§†ä½œæŠ•ç¥¨çš„ç”¨æˆ·ï¼Œä»¥å®ƒä»¬åˆ†ç±» 1 çš„æ¯”ä¾‹ä½œä¸ºå¯ä¿¡æ¦‚ç‡ä¸æ ‡ç­¾è¿›è¡Œæ¯”è¾ƒã€‚

### æ•°æ®é™ç»´

â€‹		å¯¹æ•°æ®ä½¿ç”¨ `TruncatedSVD` è¿›è¡Œé™ç»´ï¼Œä»£ç å¦‚ä¸‹ï¼š

```python
pca = TruncatedSVD(n_components=n)
train_x = pca.fit_transform(train_x)
test_x = pca.transform(test_x)
```



## Bagging ç®—æ³•çš„å®ç°

### ä»£ç æ¡†æ¶

â€‹		å°† Bagging ç®—æ³•çš„æ¡†æ¶å…‘ç°ä¸ºä»£ç å³å¯ã€‚

<img src="fig/bagging.png" style="zoom:32%;" />

```python
class Bagging:

    def __init__(self, base_algorithm, iterations, bootstrap_fraction):
        self.classifier = base_algorithm  # Class Name
        self.T = iterations
        self.F = bootstrap_fraction
        self.ensemble = []

    def fit(self, sample, label, **kwargs):
        for i in range(self.T):
            # bootstrap
            sample_i = []
            label_i = []
            for j in range(int(len(label) * self.F + 0.5)):
                id = int(random() * len(label)) - 1
                sample_i.append(sample[id])
                label_i.append(label[id])
            # train classifier
            new_classifier = self.classifier(**kwargs)
            new_classifier.fit(sample_i, label_i)
            # add classifier to ensemble
            self.ensemble.append(new_classifier)

    def predict(self, instance):
        # evaluate ensemble on instance
        res = []
        for classifier in self.ensemble:
            res.append(classifier.predict(instance))
        # obtain total vote of each class
        return np.mean(res)  # as probability of being helpful
        # output 1 == the model "votes up"
        # output 0 == the model "votes down"
        # sklearn æœ‰ä¸ª predict_prob å‡½æ•°ä¹Ÿè®¸å¯ä»¥ç”¨ï¼Ÿ

```

å€¼å¾—ä¸€æçš„æ˜¯ï¼Œè¿™ä¸ªä»£ç ä¸èƒ½å¤„ç†åˆ†è¯ç»“æœè¾“å‡ºçš„ `csr_matrix`ï¼Œå› ä¸ºå³ä½¿ä½¿ç”¨ `.getrow()` æ–¹æ³•é€‰å‡ºå„è¡Œï¼Œåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ä¾ç„¶æœ‰å„ç§é—®é¢˜å‡ºç°ã€‚æˆ‘ä½¿ç”¨çš„è§£å†³æ–¹æ³•æ˜¯è¦ä¹ˆå…ˆé™ç»´å¤„ç†ï¼Œè¦ä¹ˆä¼ å…¥ `.A` ï¼ˆå¤§æ¦‚éœ€è¦ 200G è¿è¡Œå†…å­˜å®Œæˆä¸€æ¬¡è®­ç»ƒï¼‰ï¼Œæ€»ä¹‹ç®—æ³•åªå¤„ç† `ndarray`ã€‚

â€‹		é€šè¿‡ä¸ `sklearn.ensemble.BaggingClassifier` çš„ä½¿ç”¨å¯¹æ¯”ï¼Œé™¤äº†è¿è¡Œé€Ÿåº¦çº¦æ…¢ 25% ä¹‹å¤–ï¼ŒäºŒè€…çš„æ•ˆæœç›¸è¿‘ã€‚

### è¯¯å·®åˆ†æ

â€‹		åˆ†åˆ«é‡‡ç”¨ AUC å’Œ squared error åº¦é‡å‡†ç¡®ç‡ï¼Œä»£ç å¦‚ä¸‹ï¼š

```python
bagging = Bagging(...)
bagging.fit(...)
results = {}
y_predict = [bagging.predict(sample.reshape(1, -1)) for sample in x_test]
auc = roc_auc_score([1 if y >= 0.9 else 0 for y in y_test], 
                    [1 if y >= 0.9 else 0 for y in y_predict])
err = [(pred - test) ** 2 for pred, test in zip(y_predict, y_test)]
res[i] = {bagging_svm, auc, sum(err)/len(y_test)}
```

ç”Ÿæˆæäº¤æ¯”èµ›çš„ csv æ–‡ä»¶ä»£ç å¦‚ä¸‹ï¼š

```python
bagging = Bagging(...)
bagging.fit(train_x.A, [1 if y >= 0.9 else 0 for y in train_y])

f = open('result.csv', mode='wt')
f.write('Id,Predicted\n')
for index, term in enumerate(test_x.A):
    result = bagging_svm.predict(term.reshape(1, -1))
    f.write("%d,%.2f\n" % (index, result))
f.close()
```



### å¹¶è¡ŒåŠ é€Ÿ

â€‹		è¿è¡Œè¿‡ç¨‹éœ€è¦è¿è¡Œå¤šæ¬¡æ•°æ®ç›¸äº’ç‹¬ç«‹çš„æ¨¡å‹è®­ç»ƒï¼Œæ˜¯å¦å¯ä»¥é€šè¿‡å¹¶è¡ŒåŠ é€Ÿï¼Ÿä½†æ˜¯ sklearn çš„ `SVC` ç®—æ³•æ²¡æœ‰ `n_jobs=` å‚æ•°ï¼Œåªèƒ½é€šè¿‡å¹¶è¡ŒåŒ–å¾ªç¯å®ç°ã€‚é€šè¿‡å­¦ä¹ ç¤ºä¾‹ä»£ç ï¼Œæˆ‘æ”¹å†™äº†ä¸Šè¿°ç±»ï¼š

```python
class BaggingMul(Bagging):

    def fit(self, sample, label, **kwargs):

        num_cores = multiprocessing.cpu_count()
        self.ensemble = Parallel(n_jobs=num_cores)\
            (delayed(bootstrap)(sample, label, self.classifier, 
                                self.F, **kwargs) for i in range(self.T))


def bootstrap(sample, label, classifier, F, **kwargs):
    # bootstrap
    sample_i = []
    label_i = []
    for j in range(int(len(label) * F + 0.5)):  # ä¸å¯ä»¥ç”¨ sample çš„ len
        id = int(random() * len(label)) - 1
        sample_i.append(sample[id])
        label_i.append(label[id])
    # train classifier
    new_classifier = classifier(**kwargs)
    new_classifier.fit(sample_i, label_i)
    # add classifier to ensemble
    return new_classifier
```

å°† `bootstrap` æ”¾åœ¨å…¨å±€ä½ç½®æ˜¯ä¸ºäº†ä½¿å®ƒå¯ä»¥ `pickle` åŒ–ï¼›ä½†æ˜¯è¿è¡Œæ—¶ä»ç„¶æŠ¥é”™

```pythonTraceback
_pickle.PicklingError: Could not pickle
```

é€šè¿‡æŸ¥è¯¢èµ„æ–™ï¼Œä¼¼ä¹æ˜¯å› ä¸º `bootstrap` ä¸­ä½¿ç”¨çš„ `classifier.__init__` å’Œ `fit` ä¸å¯ä»¥ `pickle` åŒ–ã€‚é‰´äº `sklearn` ä¸­çš„ `BaggingClassifier` ä¹Ÿä¸æ”¯æŒå¹¶è¡Œï¼Œåªèƒ½å¯¹æ”¯æŒå¹¶è¡Œçš„å•ä¸ªç®—æ³•å¹¶è¡Œè®­ç»ƒï¼Œè¿™ä¸ªä¼˜åŒ–èƒ½å¦å®ç°è¿˜æ˜¯ä¸€ä¸ªé—®é¢˜ã€‚è€Œè·‘åŸºäº `Decision_Tree` çš„æ¨¡å‹æ—¶ CPU è‡ªç„¶å°±èƒ½è·‘æ»¡ï¼Œä¸éœ€è¦ç‰¹åœ°è¿›è¡Œä¼˜åŒ–ã€‚

### Bagging + SVM

â€‹		æœ‰ä»¥ä¸‹å››ä¸ªå€™é€‰ SVM ç®—æ³•ï¼š`LinearSVC`ï¼Œ`SVC`ï¼Œ`LinearSVR`ï¼Œ`SVR`ã€‚å‰ä¸¤ä¸ªæ˜¯åŸºäº SVM çš„åˆ†ç±»å™¨ï¼Œåä¸¤ä¸ªæ˜¯åŸºäº SVM çš„å›å½’å™¨ã€‚`Linear...` ç‰ˆæœ¬ç›¸å½“äºé€šç”¨ç‰ˆæœ¬æŒ‡å®šçº¿æ€§ kernel çš„æƒ…å†µï¼Œä½†æ˜¯è¿è¡Œé€Ÿåº¦ä¼šå¿«ä¸€äº›ã€‚ä¾æ¬¡å°è¯•ä»¥ä¸Šå„ç§æ–¹æ³•ã€‚åˆ’åˆ†è®­ç»ƒé›†å’ŒéªŒè¯é›†å¦‚ä¸‹ï¼š

```python
RANDOM_SEED = 0xa192c122
x_train, x_test, y_train, y_test = train_test_split(train_x, train_y, test_size=0.2, random_state=RANDOM_SEED)
```

- `LinearSVC`ã€‚ä½¿ç”¨è¯¥ kernelï¼Œæ¯æ¬¡è¿­ä»£éƒ½ä¼šæŠ¥è­¦å‘Šï¼š

  ```pythonTraceback
  ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.
  ```

  è¿™è¯´æ˜çº¿æ€§ kernel ä¸å¤Ÿå¥½ã€‚

  ï¼ˆ0ï¼‰åŸºå‡†ï¼š

  - sklearn çš„ `BaggingClassifier` ä»¥é»˜è®¤å‚æ•°åˆ†ç±»åæµ‹è¯•é›† AUC ä¸º 0.5ã€‚
  - ç›´æ¥ä½¿ç”¨ LinearSVC å¾—åˆ°æµ‹è¯•é›† AUC ä¸º 0.51259ï¼Œé™ç»´è‡³ 20 åä¸º 0.46501ã€‚ 

  ï¼ˆ1ï¼‰å…ˆè®¨è®ºé™ç»´çš„å½±å“ã€‚

  - ä¸é™ç»´ï¼šè¿è¡Œæ—¶é—´éå¸¸é•¿ï¼Œé€‰å– T=50ï¼Œæµ‹è¯•é›† AUC ä¸º 0.497ã€‚
  - é™ç»´è‡³ 20ï¼šè¿è¡Œæ—¶é—´æ¯”è¾ƒçŸ­ï¼Œé€‰å– T=50ï¼Œæµ‹è¯•é›† AUC ä¸º 0.49632ã€‚

  å¯ä»¥çœ‹åˆ°é™ç»´å¯¹ AUC çš„å½±å“ä¸å¤§ï¼Œå› æ­¤é‡‡ç”¨é™ç»´åçš„æ ·æœ¬æ•°æ®è°ƒå‚ï¼Œè¿™æ ·ä¼šå¿«ä¸€ç‚¹å¾—åˆ°ç»“æœã€‚

  ï¼ˆ2ï¼‰è®¨è®º T çš„å½±å“ã€‚

  ```python
  pca = TruncatedSVD(n_components=20)
  train_x = pca.fit_transform(train_x)
  test_x = pca.transform(test_x)
  res = {}
  
  for i in range(10, 50, 5):
      print("running k = %d" % i)
      bagging_svm = Bagging(LinearSVC, i, 1)
      bagging_svm.fit(x_train, [1 if y >= 0.9 else 0 for y in y_train])
      results = {}
      y_predict = [bagging_svm.predict(test.reshape(1, -1)) for test in x_test]
      auc = roc_auc_score([1 if y >= 0.9 else 0 for y in y_test], 
                          [1 if y >= 0.9 else 0 for y in y_predict])
      err = [(pred - test) ** 2 for pred, test in zip(y_predict, y_test)]
      res[i] = {auc, sum(err)/len(y_test)}
  ```

  å¾—åˆ°çš„ç»“æœå¦‚ä¸‹ï¼š

  ```
  {
  		10: {0.5, 0.2710026340740096}, 
  		15: {0.5, 0.2518407805933097}, 
  		20: {0.5, 0.2631460849582418}, 
  		25: {0.5, 0.17070766105230506}, 
  		30: {0.5, 0.18214402200417837}, 
  		35: {0.5, 0.2689292326866453}, 
  		40: {0.5, 0.19074230169820017}, 
  		45: {0.5, 0.20343961778124936}
  }
  ```

  å¯ä»¥çœ‹åˆ°åœ¨ T=25-30 å·¦å³æœ‰æœ€ä½çš„å‡æ–¹è¯¯å·®ï¼Œä½†æ˜¯ AUC éƒ½æ˜¯ 0.5ã€‚åˆ†åˆ«ä»¥ T=20, 25, 30, 35 è®¡ç®—å¹¶æäº¤ï¼Œå¾—åˆ°æµ‹è¯•é›†ä¸Š AUC å¦‚ä¸‹ï¼š

  ```
  {
  		20: 0.51945
  		25: 0.50108
  		30: 0.46637
  		35: 0.47589
  }
  ```

  é‚£è€ƒè™‘åˆ°éšæœºå› ç´ ï¼Œè¿˜æ˜¯é€‰æ‹© T=25 æ¯”è¾ƒåˆé€‚ã€‚

  ï¼ˆ3ï¼‰è®¨è®º F çš„å½±å“ã€‚

  ```python
  for f in range(5, 15, 1):
      bagging_svm = Bagging(LinearSVC, 25, f/10)
      ...
      res[f/10] = (auc, sum(err)/len(y_test))
  ```

  ç»“æœå¦‚ä¸‹ï¼š

  ```
  {
  		0.5: (0.5, 0.25299867744504817), 
  		0.6: (0.5, 0.22400908211866713), 
  		0.7: (0.5, 0.2512126580261963), 
  		0.8: (0.5, 0.23288022489609864), 
  		0.9: (0.5, 0.30738046162947713), 
  		1.0: (0.5, 0.16901359941199012), 
  		1.1: (0.5, 0.1661272800508572), 
  		1.2: (0.5, 0.32184415846619335), 
  		1.3: (0.5, 0.16232005562577936), 
  		1.4: (0.5, 0.27464585787446916)
  }
  ```

  ä»ç»“æœå¯ä»¥çœ‹å‡ºé€‰æ‹© F=1 å³å¯ã€‚

  ï¼ˆ4ï¼‰è®¨è®º SVC å‚æ•° C çš„å½±å“ã€‚

  ```python
  for c in range(2, 20, 1):
      bagging_svm = Bagging(LinearSVC, 25, 1)
      bagging_svm.fit(x_train, 
                      [1 if y >= 0.9 else 0 for y in y_train], 
                      C=c/10)
      ...
      res[c/10] = (auc, sum(err)/len(y_test))
  ```

  ç»“æœå¦‚ä¸‹ï¼š

  ```
  {
  		0.2: (0.5, 0.2624791284236905), 
  		0.3: (0.5, 0.23319560905578776), 
  		0.4: (0.5, 0.25676870390099277), 
  		0.5: (0.5, 0.19560150748196858), 
  		0.6: (0.5, 0.19220791246582988), 
  		0.7: (0.5, 0.21228180543246514), 
  		0.8: (0.5, 0.2558191918191605), 
  		0.9: (0.5, 0.22881670286364053), 
  		1.0: (0.5, 0.2879784973526365), 
  		1.1: (0.5, 0.2020692339995148), 
  		1.2: (0.5, 0.1937371031232165), 
  		1.3: (0.5, 0.22339350760440546), 
  		1.4: (0.5, 0.18045171576669025), 
  		1.5: (0.5, 0.21743632141695027), 
  		1.6: (0.5, 0.17338010445051866), 
  		1.7: (0.5, 0.22892434173756016),
      1.8: (0.5, 0.19030924611662695), 
      1.9: (0.5, 0.1885123748725587)
  }
  ```

  è€ƒè™‘åˆ°åŸæœ¬ 1.0 çš„ç»“æœæ¯”è¾ƒå¤§ï¼Œä»¥æ­¤ä¼°ç®—éšæœºå› ç´ å½±å“å¯¼è‡´çš„å˜åŠ¨èŒƒå›´ï¼Œä¸åŒçš„ C å€¼æ²¡æœ‰æ˜¾è‘—çš„æ”¹å˜ï¼Œå¯¹ C=0.5 å’Œ C=1.5 è®¡ç®—å¹¶æäº¤å¾—åˆ° AUC å¦‚ä¸‹ï¼š

  ```
  {
  		0.5: 0.45152
  		1.5: 0.53185
  }
  ```

  ä¼¼ä¹æ›´å¤§çš„ C å€¼ç»“æœæ›´ä½³ã€‚çœ‹åˆ° 1.9 é™„è¿‘å‡æ–¹è¯¯å·®åˆé™ä½äº†ï¼Œç»§ç»­å°è¯•æ›´é«˜çš„ C å€¼ï¼š

  ```
  {
  		2.0: 0.53808
  		2.5: 0.46572
  }
  ```

  å› æ­¤è¿˜æ˜¯é€‰ C=2.0 æ¯”è¾ƒåˆé€‚ã€‚

- `SVC`ã€‚ä¸»è¦è€ƒè™‘ä½¿ç”¨ Gaussian æ ¸ã€‚å› ä¸ºå®ƒç­‰æ•ˆäºå‡ç»´ï¼Œè¿™ä¸ªæ ¸æ²¡æ³•ç¨€ç–çŸ©é˜µä½¿ç”¨ï¼Œè¦ç”¨å¿…é¡»å…ˆé™ç»´ã€‚å¹¶ä¸”å³ä½¿é™ç»´è‡³ 10 ç»´ï¼Œè¿è¡Œæ—¶é—´ä»ç„¶é•¿çš„éš¾ä»¥æ¥å—ï¼Œå¯¹å®ƒçš„è°ƒå‚è®¨è®ºå°±ä¸»è¦é›†ä¸­åœ¨åŸºåˆ†ç±»å™¨ä¸ªæ•° T ä¸Šã€‚ç»“æœå¦‚ä¸‹ï¼š

  ```
  {
  		10: (0.5, 0.49228336608415785),
  		15: (0.5, 0.49228336608415785),
  		20: (0.5, 0.49228336608415785),
  		25: (0.5, 0.49228336608415785)
  }
  ```

  å¯ä»¥çœ‹å‡ºè™½ç„¶è¿ç®—æ—¶é—´æ˜¾è‘—åŠ é•¿ï¼Œä½†æ˜¯å‡†ç¡®ç‡æ²¡æœ‰æ˜¾è‘—å˜åŒ–ã€‚å¥‡æ€ªçš„æ˜¯ï¼Œè¿™ä¸ªç»“æœä¼¼ä¹å¤±å»äº†éšæœºæ€§ï¼šä¸è®ºä½¿ç”¨å‡ æ¬¡éƒ½æ˜¯ä¸€æ¨¡ä¸€æ ·çš„æ•°å­—ã€‚

- `LinearSVR`ã€‚è¿™ä¸ªæ¨¡å‹å¯ä»¥ä¸ç”¨æŠŠ vote_up çš„æ¯”ä¾‹è°ƒæ•´æˆ 0-1 çš„ labelã€‚

  ï¼ˆ1ï¼‰è®¨è®º T å€¼çš„å½±å“ã€‚

  ```python
  for k in range(5, 31, 5):
      bagging_svm = Bagging(LinearSVR, k, 1)
      bagging_svm.fit(x_train, y_train.to_numpy(), C=1.5)
  
      y_predict = [bagging_svm.predict(test_sample.reshape(1, -1)) for test_sample in x_test]
      auc = roc_auc_score([1 if y >= 0.9 else 0 for y in y_test], [1 if y >= 0.9 else 0 for y in y_predict])
      err = [(pred - test) ** 2 for pred, test in zip(y_predict, y_test)]
      print('k=', k, ': auc=', auc, ', err=', sum(err)/len(y_test))
  ```

  å¾—åˆ°ç»“æœå¦‚ä¸‹ï¼š

  ```
  {
  		 5: (0.625933889716647, 0.09137185040673883),
  		10: (0.5371890415642789, 0.11506828137127474),
  		15: (0.5287612951114661, 0.1151155917019429),
  		20: (0.5455109431790024, 0.0728010014652953),
  		25: (0.5818876348082577, 0.07746103805359553),
  		30: (0.5257648539304445, 0.07320124685431421)
  }
  ```

  å¯ä»¥çœ‹åˆ°å°±ç›¸æ¯” SVC çš„æƒ…å†µï¼ŒSVR çš„å‡†ç¡®åº¦é«˜å¾ˆå¤šã€‚ä¸” err å’Œ AUC çš„å˜åŒ–è¶‹åŠ¿å¹¶ä¸ä¸€è‡´ã€‚æœ€ä¼˜çš„ä¼¼ä¹æ˜¯ 5ã€‚æµ‹è¯•é›†å›æŠ¥çš„ AUC å¦‚ä¸‹ï¼š

  ```
  {
  		 5: 0.56579,
  		10: 0.54927,
  		15: 0.53528,
  		20: 0.51654,
  		25: 0.48048
  }
  ```

  æ­¤å¤–ï¼ŒLinearSVR å¯èƒ½å‡ºç°å–å€¼ä¸åœ¨ 0-1 ä¹‹é—´çš„æƒ…å†µï¼Œéœ€è¦äºˆä»¥çº¦æŸã€‚

  ï¼ˆ2ï¼‰è®¨è®ºç»´åº¦çš„å½±å“ã€‚å– T=5ã€‚

  - ä¸é™ç»´ï¼šæµ‹è¯•é›†å›æŠ¥ AUC ä¸º 0.59524ã€‚

  - é™ç»´è‡³ 20ï¼šæµ‹è¯•é›†å›æŠ¥ AUC ä¸º 0.56579ã€‚

  - åœ¨å…¶é—´éšæœºé€‰æ‹©å‡ ä¸ªç»´åº¦å€¼è¿›è¡Œæµ‹è¯•ï¼š

    ```python
    for n in [50, 100, 500, 1000, 10000, 50000, 100000]:
        pca = TruncatedSVD(n_components=n)
        ...
    ```

    å¾—åˆ°éªŒè¯é›†ä¸Šç»“æœå¦‚ä¸‹ï¼š

    ```
    {
    		    50: (0.5964112735248323, 0.07706760199541933),
    		   100: (0.598365172265666, 0.10483597311412463),
    		   500: (0.6617466454154642, 0.18450958103968662), 
    		  1000: (0.5007023933571598, 0.22543262855651072)
    }
    ```

    æ›´å¤§çš„ç»´åº¦ä¼šå› ä¸ºå†…å­˜å ç”¨è¿‡å¤§è¢«ç³»ç»Ÿæ€æ­»ã€‚å– n=500 æ—¶æµ‹è¯•æ•°æ®é›†æ±‡æŠ¥çš„ AUC ä¸º 0.64219ã€‚

- `SVR` å› æ—¶é—´åŸå› ä¸å†è¿›è¡Œæµ‹è¯•ã€‚

### Bagging + DecisionTree

â€‹		åšäº†åŠå¤©ç¦»æ•£åŒ–æ‰å‘ç° sklearn çš„ DecisionTree ä¸éœ€è¦ç¦»æ•£åŒ–ã€‚ã€‚ã€‚ã€‚å› æ­¤å¯ä»¥ç›´æ¥æ²¿ç”¨ä¹‹å‰çš„ä»£ç ã€‚

```python
for i in range(10, 50, 5):
    print("running k = %d" % i)
    bagging_svm = Bagging(DecisionTreeClassifier, i, 1)
    bagging_svm.fit(x_train, np.array([1 if y >= 0.9 else 0 for y in y_train]))
    results = {}
    y_predict = [bagging_svm.predict(test.reshape(1, -1)) for test in x_test]
    auc = roc_auc_score([1 if y >= 0.9 else 0 for y in y_test],
                        [1 if y >= 0.9 else 0 for y in y_predict])
    err = [(pred - test) ** 2 for pred, test in zip(y_predict, y_test)]
    print("k=%d, auc=%f, err=%f" % (i, auc, sum(err)/len(y_test)))
```

è¾“å‡ºä¸º

```python
running k = 10
k=10, auc=0.506248, err=0.232947
running k = 15
k=15, auc=0.501842, err=0.226772
running k = 20
k=20, auc=0.502545, err=0.225503
running k = 25
k=25, auc=0.502222, err=0.223254
running k = 30
k=30, auc=0.502981, err=0.221488
running k = 35
k=35, auc=0.501215, err=0.222362
running k = 40
k=40, auc=0.501595, err=0.221613
running k = 45
k=45, auc=0.501082, err=0.220823
```

è¿™ä¸ªç»“æœå¾ˆå¥‡æ€ªï¼Œç‰¹åˆ«æ˜¯å¯¹äºåé¢å‡ ç§æƒ…å†µåœ¨éªŒè¯é›†ä¸Šçš„è¡¨ç°éƒ½ä¸å¦‚ SVMï¼Œä½†æ˜¯æµ‹è¯•é›†çš„æƒ…å†µåˆ™åä¹‹ã€‚è€Œä¸”ä¸åŒçš„å‚æ•°åœ¨éªŒè¯é›†çš„è¡¨ç°å·®åˆ«ä¸å¤§ï¼Œä¸çŸ¥é“å…¶ä¸­åŸå› ã€‚é‡æ–°æŒ‡å®šéšæœºç§å­ä¾ç„¶å¦‚æ˜¯ã€‚é‰´äºæ­¤ï¼Œæœ¬èŠ‚ä¹‹åå„é¡¹éªŒè¯éƒ½å»æµ‹è¯•é›†ä¸ŠéªŒè¯ã€‚

â€‹		è€ƒè™‘åˆ°å†³ç­–æ ‘å¹¶ä¸æ“…é•¿åˆ©ç”¨å¾ˆåˆ†æ•£çš„ä¿¡æ¯ï¼Œå°†é™ç»´çš„ç»´åº¦è¿›ä¸€æ­¥å‡å°‘ã€‚å– n=20ï¼Œk=10 å¾—åˆ°æµ‹è¯•é›†ä¸Š AUC ä¸º 0.72247ï¼Œå– k=20 å¾—åˆ°æµ‹è¯•é›†ä¸Š AUC ä¸º 0.74505ï¼Œå– k=40 å¾—åˆ°æµ‹è¯•é›†ä¸Š AUC ä¸º 0.75083ï¼Œå– k=80 å¾—åˆ°æµ‹è¯•é›†ä¸Š AUC ä¸º 0.76548ï¼Œå– k=160 å¾—åˆ°æµ‹è¯•é›†ä¸Š AUC ä¸º 0.77031ï¼Œ



## AdaBoost.M1 ç®—æ³•çš„å®ç°

### ä»£ç æ¡†æ¶

â€‹		å°† `AdaBoost.M1` ç®—æ³•çš„æ¡†æ¶å…‘ç°ä¸ºä»£ç å³å¯ã€‚

```python
class M1:

    def __init__(self, base_algorithm, iterations):
        self.classifier = base_algorithm
        self.T = iterations
        self.ensemble = []
        self._weights = []

    def fit(self, sample, label, **kwargs):
        s_weights = [1/sample.shape[0]] * sample.shape[0]  # åˆå§‹å¹³å‡æƒé‡
        for t in range(self.T):
            # Train a new learner
            classifier_i = self.classifier(**kwargs)
            classifier_i.fit(sample, label, sample_weight=s_weights)
            # Measure the Error
            pred = classifier_i.predict(sample)
            error = 0
            for wgt, lbl, pbl in zip(s_weights, label, pred):
                if (lbl - 0.9) * (pbl - 0.9) < 0:  # (1)
                    error += wgt
            if error > 0.5:
                raise RuntimeError("Choose a better classifier!")
            # Calculate classifier weight
            beta = error/(1-error)
            self.ensemble.append(classifier_i)
            self._weights.append(beta)
            # Update sample weight
            new_s_weight = []
            for wgt, lbl, pbl in zip(s_weights, label, pred):
                if (lbl - 0.9) * (pbl - 0.9) > 0:  # (2)
                    new_s_weight.append(wgt * beta)
                else:
                    new_s_weight.append(wgt)
            # Normalization
            sum_norm = sum(new_s_weight)
            s_weights = [wgt/sum_norm for wgt in new_s_weight]

  def predict(self, sample):
      # åŠ æƒå‡å€¼
      pred = 0
      for cls, wgt in zip(self.ensemble, self._weights):
      		pred += cls.predict(sample) * log(1/wgt)
      return pred
```

å…¶ä¸­ (1) ä¸ (2) å¤„çš„åˆ¤æ–­æ¡ä»¶æ˜¯ä¸ºäº†é˜²æ­¢ä½¿ç”¨ `LinearSVR` çš„è¿‡ç¨‹ä¸­é‡åˆ°é—®é¢˜ã€‚

### è¯¯å·®åˆ†æ

â€‹		å› ä¸ºä¸Šè¿° `AdaBoost.M1` æ¥å£è®¾è®¡ä¸ `Bagging` ç®—æ³•ä¸€è‡´ï¼Œå› æ­¤å¯ä½¿ç”¨åŒä¸€å¥—æ¡†æ¶è¿›è¡Œè®­ç»ƒå’Œæµ‹è¯•ã€‚

### AdaBoost.M1 + SVM

- `LinearSVC` æ— æ³•å®Œæˆæ‰§è¡Œï¼Œä¼šåœ¨è‡³å¤šç¬¬å››æ¬¡å› é”™è¯¯ç‡å¤ªé«˜é€€å‡ºã€‚ç”¨é€€å‡ºæ—¶åå°çš„åˆ†ç±»å™¨åœ¨ console ä¸­è¯„ä¼°é¢„æµ‹å€¼ï¼Œå¾—åˆ° auc ä¸º 0.48853229284676297ï¼Œsquared error ä¸º 0.44572163ï¼Œå‡æ¯”è¾ƒå·®ã€‚

-  `LinearSVR` é€šå¸¸èƒ½æ‰§è¡Œ 3-5 è½®ï¼Œé€‰å–æŸæ¬¡è¿æ°”å¥½æ‰§è¡Œå®Œ 5 è½®çš„åˆ†ç±»å™¨ï¼Œå…¶åœ¨éªŒè¯é›†ä¸Šå¹³å‡çš„ AUC ä¸º 0.60591043ï¼Œåœ¨æµ‹è¯•é›†ä¸ŠæŠ¥å‘Šçš„ AUC ä¸º 0.69031ã€‚

- æ ¹æ® Bagging ä½¿ç”¨è¿‡ç¨‹ä¸­å‘ç°çš„è§„å¾‹ï¼Œä¸¤ä¸ªå¾ˆæ…¢çš„åˆ†ç±»å™¨åªå°è¯• `SVR` ä¸€ä¸ªã€‚

  æ›´æ–°ï¼šå‰æœŸæ‹–å»¶å¤ªä¹…äº†ã€‚ã€‚ã€‚è¿™é‡Œåªè·‘ä¸€ä¸ª T=5 çš„éƒ½æ²¡èƒ½è·‘å®Œã€‚ã€‚ã€‚

### AdaBoost.M1 + DecisionTree

â€‹		æ›¿æ¢ä¸‹ä¼ ç»™ `AdaBoost.M1` çš„åˆ†ç±»å™¨ï¼Œå³å¾—åˆ°é…åˆ DecisionTree çš„ç‰ˆæœ¬ã€‚

â€‹		æ‰§è¡Œåå‡ºç° `ZeroDivisionError`ï¼ŒåŸå› æ˜¯éªŒè¯é›†ä¸Šæ‰€æœ‰åˆ†ç±»éƒ½æ­£ç¡®ï¼Œå› æ­¤æ–°çš„æƒé‡éƒ½æ˜¯ 0ï¼Œå¯¼è‡´æ­£åˆ™åŒ–è¿‡ç¨‹ä¸­å‡ºé”™ã€‚æ—¢ç„¶å‡ºç°è¿™ä¸ªæƒ…å†µï¼Œæœ‰ç†ç”±ç›¸ä¿¡å„é¡¹å‚æ•°æ¯”è¾ƒåˆé€‚ã€‚

â€‹		ä¸ºäº†é¿å…å‡ºç° NaNï¼Œç»™å†³ç­–æ ‘ä¼ å…¥æœ€å¤§æ·±åº¦ 10ï¼Œå³å¯æ­£å¸¸è¿è¡Œã€‚ä½†æ˜¯åˆ†ç±»ç»“æœå¤§éƒ¨åˆ†æ˜¯ 1ï¼Œæµ‹è¯• AUC ç»“æœä¸º 0.5007ã€‚

## æ€»ç»“è®¨è®º

- DecisionTree ç®—æ³•çš„è¡¨ç°æ˜æ˜¾ä¼˜äº SVM ç®—æ³•ã€‚
- æ„Ÿè§‰ AdaBoost.M1 ç®—æ³•çš„å®ç°è¿˜å­˜åœ¨æ½œåœ¨çš„ bugï¼ŒæŒ‰ç…§ç›´è§‰æ¥è¯´å®ƒçš„æ•ˆæœåº”è¯¥æ¯” Bagging å¥½ã€‚
- ä¸€å®šä¸èƒ½åœ¨åšå®éªŒå‰æœŸé’»ç ”äº›ä¸å¤ªé‡è¦çš„é—®é¢˜ï¼Œæ¯”å¦‚å¹¶è¡Œè®¡ç®—ï¼Œç ”ç©¶å¥½ä¹…ç ”ç©¶ä¸å‡ºæ¥ï¼Œåé¢å°±æ²¡æ—¶é—´åšäº†ğŸ¤¦â€â™‚ï¸







