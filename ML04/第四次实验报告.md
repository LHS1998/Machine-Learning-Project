# 第四次实验报告

# 一、实验目标

​		本实验以已分割好的车牌字符图像为数据集，尝试构建 KNN 模型，进行自动识别和转化。

​		本实验报告中所有代码均为示例代码，为了叙述清晰，部分是从函数、类中粘贴出来，不保证单独可以正确执行。作者所使用的环境是 MacOS 12.2.1，Python 3.9，PyCharm 2021.3.1，能正确运行各脚本。



# 二、数据描述与处理

​		本次实验的数据已经经过分割，可以直接使用。读取数据的代码如下：

```python
current_path = os.path.dirname(__file__)
train_path = os.path.join(current_path, "data", "train")
tests_path = os.path.join(current_path, "data", "test")

train_dirs = sorted([dir for dir in os.listdir(train_path) if dir.isdigit()], key=int)  # 过滤掉 .DS_Store
tests_dirs = sorted([dir for dir in os.listdir(tests_path) if dir.isdigit()], key=int)

train = []  # 考虑到训练集得打乱, 先整体读入
for name in train_dirs:
    label = name
    working_dir = os.path.join(train_path, name)
    files = os.listdir(working_dir)
    for file in files:
        file_path = os.path.join(working_dir, file)
        img = Image.open(file_path)
        pixels = np.array(img)
        train.append((int(label), pixels.flatten()))
shuffle(train)
train_data = [data for _, data in train]
train_label = [label for label, _ in train]  # 这个集合就可以直接用于交叉验证和规模改变

test_data = []
test_label = []
for name in tests_dirs:
    working_dir = os.path.join(tests_path, name)
    files = os.listdir(working_dir)
    for file in files:
        file_path = os.path.join(working_dir, file)
        img = Image.open(file_path)
        pixels = np.array(img).flatten()
        test_label.append(name)
        test_data.append(pixels)
```



# 三、KNN 分类器的实现

### 3.1 基础模型与 k 的影响

​		使用 sklearn 包中的 KNN 算法，构建模型如下。

```python
from sklearn.neighbors import KNeighborsClassifier as KNN

ks = [1, 5, 10, 20, 50, 100, 120]
for k in ks:
    model = KNN(n_neighbors=k, metric="euclidean")
    model.fit(train_data, train_label)
    prediction = model.predict(test_data)
    acc = 0
    for pred, real in zip(prediction, test_label):
        if str(pred) == real:
            acc += 1
    acc /= len(test_label)
    print(k, acc)
```

运行后得到结果如下：

```
1 0.7168274383708467
5 0.6928188638799572
10 0.6831725616291533
20 0.6615219721329046
50 0.610075026795284
100 0.5603429796355841
120 0.5479099678456592
```

由此来看 1NN 结果最好。



### 3.2 距离度量的影响

​		通过修改 KNN 初始化的 metric 参数，得到以下结果：

```
|    k   |  euclidean |  manhattan |  chebyshev |  minkowski |  minkowski |  minkowski |  minkowski |  minkowski |
|        |            |            |            |     p=1    |     p=3    |     p=4    |     p=5    |     p=20   |
------------------------------------------------------------------------------------------------------------------
|    1   |    0.7168  |    0.7145  |    0.4688  |    0.7144  |    0.7113  |    0.7014  |    0.6973  |    0.6373  |
|    5   |    0.6928  |    0.6825  |    0.4062  |    0.6825  |    0.6941  |    0.6887  |    0.6832  |    0.6433  |
|   10   |    0.6831  |    0.6718  |    0.3728  |    0.6718  |    0.6857  |    0.6793  |    0.6776  |    0.6381  |
|   20   |    0.6615  |    0.6544  |    0.3061  |    0.6454  |    0.6679  |    0.6593  |    0.6553  |    0.6109  |
|   50   |    0.6101  |    0.5969  |    0.2124  |    0.5970  |    0.6118  |    0.6116  |    0.6109  |    0.5854  |
|  100   |    0.5603  |    0.5539  |    0.1424  |    0.5539  |    0.5657  |    0.5702  |    0.5719  |    0.5578  |
|  120   |    0.5479  |    0.5449  |    0.1314  |    0.5449  |    0.5513  |    0.5518  |    0.5543  |    0.5498  |
```

可以看到欧氏距离和曼哈顿距离是比较好的选择，综合来看，还是欧氏距离更优。在各种距离度量下，都是 k 越大结果越差，但是某些度量下准确率下降地慢一些。



### 3.3 训练集大小的影响

​		我们已经对训练集做了置乱，因此可以直接从头取一部分做训练。为了便于对比，我们用使用同一次随机置乱的结果，在同一测试集进行测试。代码如下：

```python
for size in [0.2, 0.4, 0.6, 0.8, 1.]:
    print(size)
    x = int(len(train_data) * size) + 2
    train_label_ = np.array(train_label[:x])
    train_data_ = np.array(train_data[:x])
    for k in ks:
        model = KNN(n_neighbors=k, metric="euclidean")
        model.fit(train_data_, train_label_)
        prediction = model.predict(test_data)
        acc = 0
        for pred, real in zip(prediction, test_label):
            if str(pred) == real:
                acc += 1
        acc /= len(test_label)
        print(k, acc)
```

结果如下：

```
|    k   |     20%    |     40%    |     60%    |     80%    |    100%    |
---------------------------------------------------------------------------
|    1   |    0.6352  |    0.6890  |    0.6937  |    0.7059  |    0.7168  |
|    5   |    0.6090  |    0.6401  |    0.6667  |    0.6833  |    0.6928  |
|   10   |    0.5725  |    0.6169  |    0.6499  |    0.6654  |    0.6832  |
|   20   |    0.5400  |    0.5841  |    0.6184  |    0.6399  |    0.6615  |
|   50   |    0.4947  |    0.5340  |    0.5663  |    0.5921  |    0.6108  |
|  100   |    0.4300  |    0.5012  |    0.5241  |    0.5453  |    0.5603  |
|  120   |    0.4024  |    0.4935  |    0.5108  |    0.5383  |    0.5479  |
```

可以看到训练集越大则准确度越高，且 k 越大则受训练集规模的影响越大。对于 1NN，影响则不是特别显著。



### 3.4 降维的影响

​		采用 PCA 主成分分析对输入数据进行降维处理，代码如下：

```python
all_x = train_data + test_data
pca = PCA(n_components='mle', svd_solver='full')
reduced_x = pca.fit_transform(all_x)

train_data_ = reduced_x[:len(train_data)]
test_data_ = reduced_x[len(train_data):]
```

得到结论如下：

```
|    k   |     ori    |     mle    |     300    |     200    |     80%    |
---------------------------------------------------------------------------
|    1   |    0.7168  |    0.7170  |    0.7155  |    0.7149  |    0.7136  |
|    5   |    0.6928  |    0.6920  |    0.6934  |    0.6965  |    0.6881  |
|   10   |    0.6832  |    0.6831  |    0.6825  |    0.6853  |    0.6838  |
|   20   |    0.6615  |    0.6615  |    0.6623  |    0.6592  |    0.6613  |
|   50   |    0.6108  |    0.6100  |    0.6092  |    0.6111  |    0.6153  |
|  100   |    0.5603  |    0.5603  |    0.5603  |    0.5655  |    0.5715  |
|  120   |    0.5479  |    0.5479  |    0.5488  |    0.5502  |    0.5578  |
```

注：mle 算法自动选择的维度是 398。

​		由此结果来看，降维在某些 k 的取值上会优于原始模型，同时在其他的 k 上会有所损失，但是不显著。总的来说，对于特定任务，通过 PCA 进行降维是合适的。



### 3.5 加权 KNN 的影响

​		sklearn 中的 KNN 默认是采用平权模式。修改即可得到距离倒数加权的版本：

```python
model = KNN(n_neighbors=k, metric="euclidean", weights='distance')
```

相应输出为

```
1 0.7170418006430869
5 0.7016077170418007
10 0.6943193997856377
20 0.6705251875669882
50 0.6195069667738478
100 0.5749196141479099
120 0.564844587352626
```

可以看到 k 取值较大的模型准确率有了很大提升，而 1NN 的准确率则没有变化。

​		sklearn 中只有一种加权模式，如果需要其他加权模式需要自行实现函数，接受一个作为距离的 np.ndarray 并返回同样维度的表示权重的矩阵。实现高斯核函数如下：

```python
def gaussian_kernel(dis):
    sigma = 400
    func = np.vectorize(lambda d: math.exp(-pow(d/sigma, 2)))
    return func(dis)
```

准确率结果为

```
1 0.7170418006430869
5 0.7183279742765273
10 0.719828510182208
20 0.7170418006430869
50 0.7121114683815648
100 0.7099678456591639
120 0.7091103965702037
```

可以看到此时最优参数为 k=10。经过简单调参，当 sigma 值在 300-500 之间时准确率都没有太大变化。
