# 第五次实验报告

# 一、实验目标

​		本实验以 [UCI](https://archive.ics.uci.edu/ml/datasets/AAAI+2014+Accepted+Papers!) 提供的 AAAI 2014 上发表的约400篇文章为数据，设计实现或调用聚类算法对论文进行聚类。最后也可以对聚类结果进行观察，看每一类都是什么样的论文，是否有一些主题。

​		本实验报告中所有代码均为示例代码，为了叙述清晰，部分是从函数、类中粘贴出来，不保证单独可以正确执行。作者所使用的环境是 MacOS 12.2.1，Python 3.9，PyCharm 2021.3.1，能正确运行各脚本。



# 二、数据描述与处理

### 2.1 数据读入

​		因为本次实验所用数据列间关系较弱，行内关系较强，每个单元都需独立处理，因此选择采用具名元组读入 csv 文件中的信息。

```python
from collections import namedtuple
import csv

with open(csv_data, mode='rt') as f:
    csv_file = csv.reader(f)
    Record = namedtuple('Record', next(csv_file))
    records = [Record(*row) for row in csv_file]
```

### 2.2 数据预处理

​		通过观察一个典型的数据项，得到特征如下：

- 如果有多个作者，author 条目会表示成 `A, B, ... and Z` 这个形式，因此需要依据 `and` 和 `, ` 进行划分。因为作者的名字可能是有意义的单词，或者有同名的情况，因此需要作为整体使用。
- groups 和 topics 条目同时包括原文和缩写，可能具有一定的重复性，而其中缩写的特异性更高。
- topics，keywords 和 abstratc 特征中都有可能含有换行符 \n，应当处理掉。
- title, keywords 和 abstract 都是自然语言的内容，且重要性相似，但是字数差异较大，应当赋予不同的权重。
- groups 的内容可能为空。

对于处理后不影响阅读的部分，在读入过程中一并予以处理。代码如下：

```python
with open(csv_data, mode='rt') as f:
    csv_file = csv.reader(f)
    Record = namedtuple('Record', next(csv_file))
    records = []
    for row in csv_file:
        # row = [title, authors, groups, keywords, topics, abstract]
        # author应当作为一个整体加入特征, keywords 也可以考虑整体加入
        # groups 和 topics 特征有重复 (原文和缩写), 考虑选取特异性更高的缩写
        # topics 和 keywords 特征应当把 \n 处理掉
        # title, keywords 和 abstract 应当具有不同的权重
        row_ = [row[0]]
        # author 拆成人名的元组
        authors = row[1]
        if ' and ' in authors:  # 多于一个作者
            authors = authors.split(' and ')
            if ', ' in authors[0]:  # 多于两个作者
                cache = authors[1]
                authors = authors[0].split(', ')
                authors.append(cache)
        row_.append(tuple(authors))
        # group 去掉重复元素
        groups = row[2]
        groups_ = []
        if groups:  # groups 有可能为空
            if '\n' in groups:
                groups = groups.split('\n')
            else:
                groups = [groups]  # 化成字符串的列表, 以统一处理
            for term in groups:
                groups_.append(term.split("(")[1].split(")")[0])  # 取缩写
        row_.append(tuple(groups_))
        # keywords 提取
        keywords = row[3]
        if '\n' in keywords:
            keywords = keywords.split('\n')
        else:
            keywords = (keywords, )
        row_.append(tuple(keywords))
        # topics 提取
        topics = row[4]
        if '\n' in topics:
            topics = topics.split('\n')
        else:
            topics = (topics, )
        row_.append(tuple(topics))
        # abstract 可能有换行
        row_.append(row[5].replace('\n', ' '))
        # 处理完成
        records.append(Record(*row_))
```



### 2.3 特征提取

​		使用 `TfidfVectorizer` 提取特征向量。代码如下：

```python
features = []
for title, authors, groups, keywords, topics, abstract in records:
    feature = (title + ' ') * 15  # 标题权重
    for author in authors:
        feature += (author.replace(' ', '').replace('.', '') + ' ')  # 作者名字作为一个整体加入
    for group in groups:
        feature += (group + ' ')
    for keyword in keywords:
        feature += (keyword + ' ') * 20  # 关键字权重
    for topic in topics:
        feature += (topic + ' ')  # 有待改进
    feature += abstract
    features.append(feature)

del title, authors, groups, keywords, topics, abstract, author, feature, group, keyword, topic

vectorizer = TfidfVectorizer()
vectorizer.fit(features)
vec = vectorizer.transform(features)
```

通过重复标题和关键字提升其权重，使标题、关键字和摘要量级相同。以标题 10-20 词、关键字 5 个每个 2-3 个词、摘要 150-300 词进行估计，选取标题权重为 15，关键字权重为 20。作者名字去除干净空格和省略点号，使之成为一个词。当加和各特征时，需要添加空格，防止多个词连成一片。



# 三、聚类实现

### 3.1 K-Medoids

​		调用已有的库。输出类和文章名称。

```python
from sklearn_extra.cluster import KMedoids

num = 10
way = 'K-Medoiods'
cluster = KMedoids(n_clusters=num)
cluster.fit(vec)

result = [[] for i in range(num)]
for rec, lbl in zip(records, cluster.labels_):
    result[lbl].append(rec)

with open("{}-{}-weighted.txt".format(way, num), mode='wt') as f:
    for i, rec in enumerate(result):
        f.write('==== Cluster {} ===>\n'.format(i+1))
        for j, term in enumerate(rec):
            f.write('\t({}) {}'.format(j+1, term[0]))
            f.write('\n')
        f.write('\n\n')
```

对比未对标题、关键字加权和已加权的情况，对于已加权情况，尝试多种类别数目。输出结果分别保存在 results 内相应文件夹下。下示 `num=25` 时前三个簇：

```
==== Cluster 1 ===>
	(1) Constructing Symbolic Representations for High-Level Planning
	(2) Robust Bayesian Inverse Reinforcement Learning with Sparse Behavior Noise
	(3) Symbolic Domain Predictive Control
	(4) Learning Low-Rank Representations with Classwise Block-Diagonal Structure for Robust Face Recognition
	(5) Structured Possibilistic Planning using Decision Diagrams
	(6) Imitation Learning with Demonstrations and Shaping Rewards
	(7) Effective Management of Electric Vehicle Storage using Smart Charging
	(8) Learning Word Representation Considering Proximity and Ambiguity
	(9) Decentralized Multi-Agent Reinforcement Learning in Average-Reward Dynamic DCOPs
	(10) Pathway Specification and Comparative Queries: A High Level Language with Petri Net Semantics
	(11) Mixing-time Regularized Policy Gradient
	(12) An Agent-Based Model Studying the Acquisition of a Language System of Logical Constructions
	(13) Tree-Based On-line Reinforcement Learning
	(14) Natural Temporal Difference Learning


==== Cluster 2 ===>
	(1) Kernelized Bayesian Transfer Learning
	(2) Predicting the Hardness of Learning Bayesian Networks
	(3) Small-variance Asymptotics for Dirichlet Process Mixtures of SVMs
	(4) Using The Matrix Ridge Approximation to Speedup Determinantal Point Processes Sampling Algorithms
	(5) Wormhole Hamiltonian Monte Carlo
	(6) A Characterization of the Single-Peaked Single-Crossing Domain
	(7) Latent Domains Modeling for Domain Adaptation
	(8) Improving Domain-independent Cloud-based Speech Recognition with Domain-dependent Phonetic Post-processing
	(9) A Spatially Sensitive Kernel to Predict Cognitive Performance from Short-Term Changes in Neural Structure
	(10) Sample-Adaptive Multiple Kernel Learning
	(11) Finding the k-best Equivalence Classes of Bayesian Network Structures for Model Averaging
	(12) Monte Carlo Filtering using Kernel Embedding of Distributions
	(13) Tightening Bounds for Bayesian Network Structure Learning
	(14) locality preserving projection via multi-objective learning for domain adaptation
	(15) Propagating Regular Counting Constraints
	(16) Point-based POMDP solving with factored value function approximation


==== Cluster 3 ===>
	(1) Locality Preserving Hashing
	(2) Supervised Hashing for Image Retrieval via Image Representation Learning
	(3) Sub-Selective Quantization for Large-Scale Image Search
	(4) Exponential Deepening A* for Real-Time Agent-Centered Search
	(5) Scheduling for Transfers in Pickup and Delivery Problems with Very Large Neighborhood Search
	(6) A Joint Optimization Model for Image Summarization Based on Image Content and Tags
	(7) Parallel Restarted Search
	(8) Large-Scale Supervised Multimodal Hashing with Semantic Correlation Maximization
	(9) Simpler Bounded Suboptimal Search
	(10) DJAO: A Communication-Constrained DCOP algorithm that combines features of ADOPT and Action-GDL


==== Cluster 4 ===>
	...
```

通过简单的标题阅读，能感到簇数目越多，则主题越清晰，在上述聚类结果中已有比较明确的主题了。此外，标题和关键字加权后这个主题更加明确。



### 3.2 K-Means

​		只需更换 3.1 节代码中前六行即可使用 K-Means 方法：

```python
from sklearn.cluster import KMeans

num = 25
way = 'K-Means'
cluster = KMeans(n_clusters=num)
cluster.fit(vec)
```

通过人工阅读，结果的簇的主题似乎更加明确。再尝试 MiniBatch K-Means：

```python
from sklearn.cluster import MiniBatchKMeans

num = 25
way = 'MiniBatch-K-Means'
cluster = MiniBatchKMeans(n_clusters=num)
cluster.fit(vec)
```

这个结果生成了非常大的单个簇，感觉并非很合适。





























